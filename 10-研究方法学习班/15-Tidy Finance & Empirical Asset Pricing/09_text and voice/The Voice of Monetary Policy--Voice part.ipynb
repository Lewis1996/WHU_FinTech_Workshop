{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8374af54",
   "metadata": {},
   "source": [
    "# The Voice of Monetary Policy--Voice part\n",
    "2024.01.08  石宛青"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daceaa",
   "metadata": {},
   "source": [
    "## 目的：识别美联储FOMC发布会音频的情绪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0ebf8",
   "metadata": {},
   "source": [
    "### 框架：获取音频数据-提取声音特征-训练模型-预测FOMC情绪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25578c6c",
   "metadata": {},
   "source": [
    "#### （1）获取音频数据\n",
    "\n",
    " 1. Ryerson Audio-Visual Database of Emotional Speech and Song（RAVDESS）  \n",
    " 该数据集包括24名专业演员（12男，12女），以中性的北美口音说出两个词法匹配的陈述，情绪包括平静、快乐、悲伤、恐惧、愤怒、惊讶、厌恶，每个表情都是在两个层次的情绪强度下产生的。\n",
    "\n",
    "    文件名标识符：e.g., 03-01-06-01-02-01-12.wav)\n",
    "\n",
    "     模态（01 = 全 AV，02 = 纯视频，03 = 纯音频）。  \n",
    "     声道（01 = 语音，02 = 歌曲）。  \n",
    "     情绪（01 =中性，02 =平静，03 =快乐，04 =悲伤，05 =愤怒，06 =恐惧，07 =厌恶，08 =惊讶）。  \n",
    "     情绪强度（01 =正常，02 =强烈）。注意：“中性”情绪没有强烈的强度。  \n",
    "     语句（01 = “孩子们在门边说话”，02 = “狗坐在门边”）(\"Kids are talking by the door\", \"Dogs are sitting by the door\")  \n",
    "     重复（01 = 第一次重复，02 = 第二次重复）。  \n",
    "     演员（01至24。奇数演员是男性，偶数演员是女性）。   \n",
    "\n",
    "\n",
    " 2. Toronto emotional speech set（TESS）  \n",
    " \n",
    "     文件名标识符：e.g., 03-01-06-back-26.wav)\n",
    "\n",
    "     模态（01 = 全 AV，02 = 纯视频，03 = 纯音频）。    \n",
    "     声道（01 = 语音，02 = 歌曲）。    \n",
    "     情绪（01 =中性，03 =快乐，04 =悲伤，05 =愤怒，06 =恐惧，07 =厌恶，08 =惊讶）。    \n",
    "     词（in total 200 target words)      \n",
    "     演员（26和28）。  \n",
    "\n",
    "\n",
    " 3. FOMC audio \n",
    "     3.1 从Youtube下载视频，网址：(https://www.youtube.com/watch?v=HdV2VUNh4E&list=PL159CD41EB36CFE86&ab_channel=FederalReserve）\n",
    "     平均55分钟，使用插件下可以不需要会员（https://www.gihosoft.com/free-youtube-downloader.html）,下载格式mp4\n",
    "\n",
    "     3.2 手动切分为开场白，问题、答案。如20190731可切割为一段开场白，24个问题，24段答案。平均1-2min。  \n",
    "     3.3 将视频转化为音频，MP4→wav (https://www.aconvert.com/audio/)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6247224",
   "metadata": {},
   "source": [
    "#### （2）提取声学特征\n",
    " 1. 将 .wav   文件转换为单声道、16000Hz格式\n",
    " 2. 从每一个音频文件中提取大量声学特征\n",
    "2.1 mfcc（40）音频信号  \n",
    "2.2 chroma（12）音调  \n",
    "2.3 mel（128）音频信号的梅尔频谱图  \n",
    "2.4 contrast（7）描述音频频谱中频段之间对比度的特征。如区分清晰的音乐和嘈杂的环境声音。  \n",
    "2.5 tonnetz（6）是音频的音高特征，表示音频信号中和声音高有关的信息。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85ae31",
   "metadata": {},
   "source": [
    "#### （3）训练模型\n",
    " 1. 划分训练集、测试集  \n",
    "将RAVDESS、TESS特征数据中的80%作为训练样本、20%作为测试集\n",
    " 2. 神经网络模型  \n",
    "180声音特征→200节点（线性激活）→200节点→分类：5种情绪  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78a893",
   "metadata": {},
   "source": [
    "#### （4）预测  \n",
    "  输入FOMC每段回答的音频特征，使用模型预测情感  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e969fb",
   "metadata": {},
   "source": [
    "### 代码：处理音频数据-模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a13e9",
   "metadata": {},
   "source": [
    "### （1）处理音频数据\n",
    "引入包。版本要求python3.6-3.9（tensorflow目前支持的版本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9b71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f059278",
   "metadata": {},
   "source": [
    "首先，定义了一个名为convert_audios的函数，用于将音频文件转换为指定的格式（单声道，采样率为16,000Hz）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994fac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audios(path, target_path):\n",
    "    for dirpath, _, filenames in os.walk(path):#使用os.walk遍历指定路径下的所有文件，找到以.wav结尾的文件，\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirpath, filename).replace('\\\\','/')\n",
    "            if file.endswith(\".wav\"):\n",
    "                target_file = target_path+'/'+filename\n",
    "                if not os.path.isfile(target_file):#如果目标文件已经存在，则跳过转换。\n",
    "                    command = f\"ffmpeg -i {file} -ac 1 -ar 16000 {target_file}\"#使用FFmpeg将其转换为单声道、采样率为16,000Hz的格式。\n",
    "                    subprocess.call(command, shell=True)\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7844560",
   "metadata": {},
   "source": [
    "接下来，定义了一个名为extract_feature的函数，用于从音频文件中提取声学特征：  \n",
    "该函数使用librosa库读取音频文件，然后提取MFCC、Chroma、MEL频谱频率、Contrast和Tonnetz等特征。提取的特征被组织成DataFrame，并返回给调用者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b21e620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    '''\n",
    "        Extract the following features\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        Not sure if others are useful: spectral_centroid, flatness, rolloff, etc.\n",
    "    '''\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        #Short-time Fourier transform\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        #mfcc\n",
    "        #音频信号\n",
    "        mfccs = np.mean(librosa.feature.mfcc(S=stft, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        mfccs_df = DataFrame(mfccs.reshape(-1, len(mfccs)))\n",
    "        mfccs_df.columns=['mfccs'+str(i) for i in range(0,len(mfccs))]\n",
    "        #chroma音调\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=X, sr=sample_rate).T,axis=0)\n",
    "        chroma_df = DataFrame(chroma.reshape(-1, len(chroma)))\n",
    "        chroma_df.columns=['chroma'+str(i) for i in range(0,len(chroma))]\n",
    "        #mel音频信号的梅尔频谱图\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "        mel_df = DataFrame(mel.reshape(-1, len(mel)))\n",
    "        mel_df.columns=['mel'+str(i) for i in range(0,len(mel))]\n",
    "        #contrast\n",
    "        # contrast 是一个描述音频频谱中频段之间对比度的特征。它测量了每个频段相对于整体频谱的对比度，以捕捉音频中的频谱特征。对比度特征可能有助于区分不同类型的声音，例如区分清晰的音乐和嘈杂的环境声音。\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        contrast_df = DataFrame(contrast.reshape(-1, len(contrast)))\n",
    "        contrast_df.columns=['contrast'+str(i) for i in range(0,len(contrast))]\n",
    "        #tonnetz\n",
    "        #Tonnetz 是音频的音高特征，表示音频信号中和声音高有关的信息。\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        tonnetz_df = DataFrame(tonnetz.reshape(-1, len(tonnetz)))\n",
    "        tonnetz_df.columns=['tonnetz'+str(i) for i in range(0,len(tonnetz))]\n",
    "    return mfccs_df, chroma_df, mel_df, contrast_df, tonnetz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d57cd3",
   "metadata": {},
   "source": [
    "在https://blog.csdn.net/m0_47449768/article/details/130102406 下载ffmpeg软件，并配置path环境变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589786e2",
   "metadata": {},
   "source": [
    "手动创建一个空的目标文件夹/traget/training_data，将RAVDESS、RAVDESS转化为目标格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44c9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = 'D:/voice/RAVDESS'\n",
    "target_path = 'D:/voice/traget/training_data'\n",
    "convert_audios(original_path, target_path)\n",
    "#约3min\n",
    "original_path = 'D:/voice/TESS'\n",
    "target_path = 'D:/voice/traget/training_data'\n",
    "convert_audios(original_path, target_path)\n",
    "#注意：文件名不能有中文、空格等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350306de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = DataFrame()\n",
    "\n",
    "filenames = glob.glob(target_path+'/'+'*.wav')#原文少了一个+'/'\n",
    "for filename in filenames:\n",
    "    mfccs, chroma, mel, contrast, tonnetz = extract_feature(filename)\n",
    "    rows = pd.concat([mfccs, chroma, mel, contrast, tonnetz], axis=1)\n",
    "\n",
    "    # 从文件名中提取情感标签（对于训练数据）和音频文件标识符（对于预测数据）\n",
    "    if str(filename).find('training_data') != -1:\n",
    "        #原文：emotion = filename.split('_')[-1].replace('.wav', '')\n",
    "        emotion = filename.split('-')[2]\n",
    "        rows['emotion'] = np.array(emotion)\n",
    "    elif str(filename).find('prediction_data') != -1:\n",
    "        fname = filename.split('\\\\')[-1].replace('.wav', '')#原文是/,不太对\n",
    "        rows['item'] = np.array(fname)\n",
    "    #原文final_df = final_df.append(rows)\n",
    "    final_df = pd.concat([final_df, rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d07915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('D:/voice/feature.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "894dd50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfccs0</th>\n",
       "      <th>mfccs1</th>\n",
       "      <th>mfccs2</th>\n",
       "      <th>mfccs3</th>\n",
       "      <th>mfccs4</th>\n",
       "      <th>mfccs5</th>\n",
       "      <th>mfccs6</th>\n",
       "      <th>mfccs7</th>\n",
       "      <th>mfccs8</th>\n",
       "      <th>mfccs9</th>\n",
       "      <th>...</th>\n",
       "      <th>contrast4</th>\n",
       "      <th>contrast5</th>\n",
       "      <th>contrast6</th>\n",
       "      <th>tonnetz0</th>\n",
       "      <th>tonnetz1</th>\n",
       "      <th>tonnetz2</th>\n",
       "      <th>tonnetz3</th>\n",
       "      <th>tonnetz4</th>\n",
       "      <th>tonnetz5</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.022527</td>\n",
       "      <td>0.958868</td>\n",
       "      <td>0.494029</td>\n",
       "      <td>0.307909</td>\n",
       "      <td>0.132615</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>-0.053159</td>\n",
       "      <td>-0.073568</td>\n",
       "      <td>-0.080976</td>\n",
       "      <td>-0.041067</td>\n",
       "      <td>...</td>\n",
       "      <td>17.426861</td>\n",
       "      <td>17.076383</td>\n",
       "      <td>15.520328</td>\n",
       "      <td>-0.042179</td>\n",
       "      <td>0.034978</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>-0.042395</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288416</td>\n",
       "      <td>0.311845</td>\n",
       "      <td>0.235072</td>\n",
       "      <td>0.202644</td>\n",
       "      <td>0.187299</td>\n",
       "      <td>0.173715</td>\n",
       "      <td>0.143612</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.114782</td>\n",
       "      <td>0.088920</td>\n",
       "      <td>...</td>\n",
       "      <td>15.573664</td>\n",
       "      <td>15.925498</td>\n",
       "      <td>15.141866</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.015157</td>\n",
       "      <td>-0.063598</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.406408</td>\n",
       "      <td>0.329117</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>0.182976</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.107178</td>\n",
       "      <td>0.062055</td>\n",
       "      <td>0.030202</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>...</td>\n",
       "      <td>17.362159</td>\n",
       "      <td>16.831559</td>\n",
       "      <td>16.501101</td>\n",
       "      <td>-0.055107</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>-0.087559</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.035862</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.474276</td>\n",
       "      <td>1.524645</td>\n",
       "      <td>0.974791</td>\n",
       "      <td>0.732221</td>\n",
       "      <td>0.644528</td>\n",
       "      <td>0.569810</td>\n",
       "      <td>0.358410</td>\n",
       "      <td>0.111121</td>\n",
       "      <td>-0.048382</td>\n",
       "      <td>-0.038311</td>\n",
       "      <td>...</td>\n",
       "      <td>16.342105</td>\n",
       "      <td>17.730312</td>\n",
       "      <td>14.985024</td>\n",
       "      <td>-0.021259</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>-0.040669</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>-0.014577</td>\n",
       "      <td>-0.014232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.522091</td>\n",
       "      <td>0.463887</td>\n",
       "      <td>0.341587</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>0.180913</td>\n",
       "      <td>0.114630</td>\n",
       "      <td>0.026273</td>\n",
       "      <td>0.023030</td>\n",
       "      <td>-0.018288</td>\n",
       "      <td>-0.055002</td>\n",
       "      <td>...</td>\n",
       "      <td>18.062980</td>\n",
       "      <td>17.388405</td>\n",
       "      <td>16.019585</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>-0.028387</td>\n",
       "      <td>0.082039</td>\n",
       "      <td>0.012937</td>\n",
       "      <td>0.037996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>8.490258</td>\n",
       "      <td>7.653400</td>\n",
       "      <td>4.493712</td>\n",
       "      <td>2.629095</td>\n",
       "      <td>1.369422</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>-1.275612</td>\n",
       "      <td>-0.488112</td>\n",
       "      <td>-0.736113</td>\n",
       "      <td>-0.736116</td>\n",
       "      <td>...</td>\n",
       "      <td>21.225740</td>\n",
       "      <td>22.143100</td>\n",
       "      <td>15.800582</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>-0.004746</td>\n",
       "      <td>-0.030455</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>0.026051</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>6.096761</td>\n",
       "      <td>5.610900</td>\n",
       "      <td>4.078144</td>\n",
       "      <td>2.305208</td>\n",
       "      <td>0.619494</td>\n",
       "      <td>0.451949</td>\n",
       "      <td>0.301742</td>\n",
       "      <td>0.249073</td>\n",
       "      <td>-0.063706</td>\n",
       "      <td>-0.400599</td>\n",
       "      <td>...</td>\n",
       "      <td>20.431165</td>\n",
       "      <td>19.394712</td>\n",
       "      <td>17.667970</td>\n",
       "      <td>0.070486</td>\n",
       "      <td>0.102015</td>\n",
       "      <td>0.087497</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.043016</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>9.083913</td>\n",
       "      <td>7.782488</td>\n",
       "      <td>5.288871</td>\n",
       "      <td>3.342988</td>\n",
       "      <td>2.207076</td>\n",
       "      <td>1.257422</td>\n",
       "      <td>0.055670</td>\n",
       "      <td>-0.548027</td>\n",
       "      <td>-0.770778</td>\n",
       "      <td>-0.670887</td>\n",
       "      <td>...</td>\n",
       "      <td>17.811781</td>\n",
       "      <td>18.966005</td>\n",
       "      <td>15.217570</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>0.057701</td>\n",
       "      <td>-0.004201</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>4.847407</td>\n",
       "      <td>4.084107</td>\n",
       "      <td>3.695012</td>\n",
       "      <td>2.926731</td>\n",
       "      <td>1.594043</td>\n",
       "      <td>1.437396</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.532656</td>\n",
       "      <td>0.077507</td>\n",
       "      <td>-0.040866</td>\n",
       "      <td>...</td>\n",
       "      <td>19.969279</td>\n",
       "      <td>19.172665</td>\n",
       "      <td>16.683653</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.056708</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>5.359800</td>\n",
       "      <td>5.321207</td>\n",
       "      <td>4.844370</td>\n",
       "      <td>4.106940</td>\n",
       "      <td>3.183647</td>\n",
       "      <td>2.994763</td>\n",
       "      <td>2.367338</td>\n",
       "      <td>2.102175</td>\n",
       "      <td>1.532907</td>\n",
       "      <td>1.082059</td>\n",
       "      <td>...</td>\n",
       "      <td>19.499500</td>\n",
       "      <td>19.602591</td>\n",
       "      <td>17.277334</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.064002</td>\n",
       "      <td>-0.039156</td>\n",
       "      <td>0.026176</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>-0.008780</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4239 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mfccs0    mfccs1    mfccs2    mfccs3    mfccs4    mfccs5    mfccs6  \\\n",
       "0     1.022527  0.958868  0.494029  0.307909  0.132615  0.016355 -0.053159   \n",
       "1     0.288416  0.311845  0.235072  0.202644  0.187299  0.173715  0.143612   \n",
       "2     0.406408  0.329117  0.183439  0.182976  0.133141  0.107178  0.062055   \n",
       "3     1.474276  1.524645  0.974791  0.732221  0.644528  0.569810  0.358410   \n",
       "4     0.522091  0.463887  0.341587  0.267636  0.180913  0.114630  0.026273   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4234  8.490258  7.653400  4.493712  2.629095  1.369422 -0.511180 -1.275612   \n",
       "4235  6.096761  5.610900  4.078144  2.305208  0.619494  0.451949  0.301742   \n",
       "4236  9.083913  7.782488  5.288871  3.342988  2.207076  1.257422  0.055670   \n",
       "4237  4.847407  4.084107  3.695012  2.926731  1.594043  1.437396  0.809804   \n",
       "4238  5.359800  5.321207  4.844370  4.106940  3.183647  2.994763  2.367338   \n",
       "\n",
       "        mfccs7    mfccs8    mfccs9  ...  contrast4  contrast5  contrast6  \\\n",
       "0    -0.073568 -0.080976 -0.041067  ...  17.426861  17.076383  15.520328   \n",
       "1     0.137624  0.114782  0.088920  ...  15.573664  15.925498  15.141866   \n",
       "2     0.030202 -0.004246 -0.001801  ...  17.362159  16.831559  16.501101   \n",
       "3     0.111121 -0.048382 -0.038311  ...  16.342105  17.730312  14.985024   \n",
       "4     0.023030 -0.018288 -0.055002  ...  18.062980  17.388405  16.019585   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "4234 -0.488112 -0.736113 -0.736116  ...  21.225740  22.143100  15.800582   \n",
       "4235  0.249073 -0.063706 -0.400599  ...  20.431165  19.394712  17.667970   \n",
       "4236 -0.548027 -0.770778 -0.670887  ...  17.811781  18.966005  15.217570   \n",
       "4237  0.532656  0.077507 -0.040866  ...  19.969279  19.172665  16.683653   \n",
       "4238  2.102175  1.532907  1.082059  ...  19.499500  19.602591  17.277334   \n",
       "\n",
       "      tonnetz0  tonnetz1  tonnetz2  tonnetz3  tonnetz4  tonnetz5  emotion  \n",
       "0    -0.042179  0.034978  0.012448 -0.042395  0.004703 -0.001524        1  \n",
       "1    -0.000143 -0.015157 -0.063598  0.014533  0.012302  0.001000        1  \n",
       "2    -0.055107  0.034173 -0.087559  0.015745  0.035862  0.000829        1  \n",
       "3    -0.021259  0.054719 -0.040669  0.023162 -0.014577 -0.014232        1  \n",
       "4     0.010363  0.023506 -0.028387  0.082039  0.012937  0.037996        1  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "4234  0.046143 -0.004746 -0.030455 -0.014472  0.026051  0.013821        8  \n",
       "4235  0.070486  0.102015  0.087497  0.000688  0.043016 -0.003614        8  \n",
       "4236  0.012104 -0.000034 -0.039569  0.057701 -0.004201  0.004223        8  \n",
       "4237 -0.003896  0.028683  0.056708  0.004761  0.017381 -0.011286        8  \n",
       "4238  0.005944  0.064002 -0.039156  0.026176  0.007878 -0.008780        8  \n",
       "\n",
       "[4239 rows x 194 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a50eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/voice/feature.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5640ad",
   "metadata": {},
   "source": [
    "手动创建一个空的目标文件夹/traget/prediction_data，将FOMC转化为目标格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ba8a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = r'D:\\voice\\FOMC'\n",
    "target_path = r'D:\\voice\\target\\prediction_data'\n",
    "convert_audios(original_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a6c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = DataFrame()\n",
    "\n",
    "filenames = glob.glob(target_path+'/'+'*.wav')#原文少了一个+'/'\n",
    "for filename in filenames:\n",
    "    mfccs, chroma, mel, contrast, tonnetz = extract_feature(filename)\n",
    "    rows = pd.concat([mfccs, chroma, mel, contrast, tonnetz], axis=1)\n",
    "\n",
    "    # 从文件名中提取情感标签（对于训练数据）和音频文件标识符（对于预测数据）\n",
    "    if str(filename).find('training_data') != -1:\n",
    "        #原文：emotion = filename.split('_')[-1].replace('.wav', '')\n",
    "        emotion = filename.split('-')[2]\n",
    "        rows['emotion'] = np.array(emotion)\n",
    "    elif str(filename).find('prediction_data') != -1:\n",
    "        fname = filename.split('\\\\')[-1].replace('.wav', '')#原文是/,不太对\n",
    "        rows['item'] = np.array(fname)\n",
    "    \n",
    "    #final_df = final_df.append(rows)\n",
    "    final_df = pd.concat([final_df, rows], ignore_index=True)\n",
    "    \n",
    "final_df.to_csv('D:/voice/feature_prediction.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dfe5c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfccs0</th>\n",
       "      <th>mfccs1</th>\n",
       "      <th>mfccs2</th>\n",
       "      <th>mfccs3</th>\n",
       "      <th>mfccs4</th>\n",
       "      <th>mfccs5</th>\n",
       "      <th>mfccs6</th>\n",
       "      <th>mfccs7</th>\n",
       "      <th>mfccs8</th>\n",
       "      <th>mfccs9</th>\n",
       "      <th>...</th>\n",
       "      <th>contrast4</th>\n",
       "      <th>contrast5</th>\n",
       "      <th>contrast6</th>\n",
       "      <th>tonnetz0</th>\n",
       "      <th>tonnetz1</th>\n",
       "      <th>tonnetz2</th>\n",
       "      <th>tonnetz3</th>\n",
       "      <th>tonnetz4</th>\n",
       "      <th>tonnetz5</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.520031</td>\n",
       "      <td>4.823277</td>\n",
       "      <td>2.519659</td>\n",
       "      <td>1.375790</td>\n",
       "      <td>1.250551</td>\n",
       "      <td>1.212689</td>\n",
       "      <td>0.507529</td>\n",
       "      <td>0.363239</td>\n",
       "      <td>0.463409</td>\n",
       "      <td>0.474395</td>\n",
       "      <td>...</td>\n",
       "      <td>16.018960</td>\n",
       "      <td>17.639429</td>\n",
       "      <td>15.605247</td>\n",
       "      <td>-0.042270</td>\n",
       "      <td>-0.014227</td>\n",
       "      <td>-0.020699</td>\n",
       "      <td>-0.038659</td>\n",
       "      <td>-0.022382</td>\n",
       "      <td>-0.003938</td>\n",
       "      <td>20110622_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.383839</td>\n",
       "      <td>8.365168</td>\n",
       "      <td>4.148173</td>\n",
       "      <td>1.992658</td>\n",
       "      <td>1.836000</td>\n",
       "      <td>1.885299</td>\n",
       "      <td>0.919860</td>\n",
       "      <td>0.648280</td>\n",
       "      <td>0.614450</td>\n",
       "      <td>0.432805</td>\n",
       "      <td>...</td>\n",
       "      <td>17.178497</td>\n",
       "      <td>20.117435</td>\n",
       "      <td>15.817274</td>\n",
       "      <td>-0.011430</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>-0.017634</td>\n",
       "      <td>-0.051248</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>-0.004221</td>\n",
       "      <td>20110622_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.130367</td>\n",
       "      <td>7.069550</td>\n",
       "      <td>3.839147</td>\n",
       "      <td>1.882912</td>\n",
       "      <td>1.592320</td>\n",
       "      <td>1.494450</td>\n",
       "      <td>0.501571</td>\n",
       "      <td>0.352009</td>\n",
       "      <td>0.417230</td>\n",
       "      <td>0.490138</td>\n",
       "      <td>...</td>\n",
       "      <td>17.335025</td>\n",
       "      <td>19.589634</td>\n",
       "      <td>15.808904</td>\n",
       "      <td>-0.025843</td>\n",
       "      <td>-0.006595</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>-0.010676</td>\n",
       "      <td>-0.012452</td>\n",
       "      <td>20110622_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.547326</td>\n",
       "      <td>8.959697</td>\n",
       "      <td>3.605210</td>\n",
       "      <td>1.777991</td>\n",
       "      <td>2.072085</td>\n",
       "      <td>2.201066</td>\n",
       "      <td>0.990699</td>\n",
       "      <td>0.778654</td>\n",
       "      <td>1.064813</td>\n",
       "      <td>1.097703</td>\n",
       "      <td>...</td>\n",
       "      <td>17.349606</td>\n",
       "      <td>21.141986</td>\n",
       "      <td>16.211789</td>\n",
       "      <td>-0.020132</td>\n",
       "      <td>-0.004959</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>-0.008769</td>\n",
       "      <td>-0.009025</td>\n",
       "      <td>20110622_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.259261</td>\n",
       "      <td>7.962046</td>\n",
       "      <td>4.193980</td>\n",
       "      <td>2.526863</td>\n",
       "      <td>1.938031</td>\n",
       "      <td>1.618798</td>\n",
       "      <td>0.947708</td>\n",
       "      <td>0.770062</td>\n",
       "      <td>0.732871</td>\n",
       "      <td>0.404933</td>\n",
       "      <td>...</td>\n",
       "      <td>17.420084</td>\n",
       "      <td>19.161898</td>\n",
       "      <td>15.782131</td>\n",
       "      <td>-0.018666</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>-0.034765</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>20110622_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.772644</td>\n",
       "      <td>3.019469</td>\n",
       "      <td>1.969844</td>\n",
       "      <td>1.756690</td>\n",
       "      <td>1.475093</td>\n",
       "      <td>1.313248</td>\n",
       "      <td>0.922565</td>\n",
       "      <td>0.946543</td>\n",
       "      <td>0.689499</td>\n",
       "      <td>0.482501</td>\n",
       "      <td>...</td>\n",
       "      <td>16.633047</td>\n",
       "      <td>17.794529</td>\n",
       "      <td>16.157175</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>-0.023302</td>\n",
       "      <td>-0.014350</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>20190731_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.260175</td>\n",
       "      <td>4.152248</td>\n",
       "      <td>2.736296</td>\n",
       "      <td>2.672591</td>\n",
       "      <td>2.143484</td>\n",
       "      <td>1.869599</td>\n",
       "      <td>1.326102</td>\n",
       "      <td>1.226196</td>\n",
       "      <td>0.776981</td>\n",
       "      <td>0.426033</td>\n",
       "      <td>...</td>\n",
       "      <td>16.435273</td>\n",
       "      <td>17.839591</td>\n",
       "      <td>16.444682</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.016555</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>-0.007346</td>\n",
       "      <td>20190731_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.875129</td>\n",
       "      <td>4.165122</td>\n",
       "      <td>2.616739</td>\n",
       "      <td>2.305677</td>\n",
       "      <td>1.956817</td>\n",
       "      <td>1.802361</td>\n",
       "      <td>1.360197</td>\n",
       "      <td>1.336472</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>0.582678</td>\n",
       "      <td>...</td>\n",
       "      <td>16.631618</td>\n",
       "      <td>18.058242</td>\n",
       "      <td>16.392727</td>\n",
       "      <td>0.015156</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>-0.011342</td>\n",
       "      <td>-0.016616</td>\n",
       "      <td>-0.007998</td>\n",
       "      <td>20190731_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.667109</td>\n",
       "      <td>4.149654</td>\n",
       "      <td>2.780008</td>\n",
       "      <td>2.448769</td>\n",
       "      <td>1.989738</td>\n",
       "      <td>1.848565</td>\n",
       "      <td>1.424531</td>\n",
       "      <td>1.212157</td>\n",
       "      <td>0.929775</td>\n",
       "      <td>0.670839</td>\n",
       "      <td>...</td>\n",
       "      <td>16.803352</td>\n",
       "      <td>18.052364</td>\n",
       "      <td>16.445027</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>-0.034944</td>\n",
       "      <td>-0.020906</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>20190731_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.967470</td>\n",
       "      <td>3.745574</td>\n",
       "      <td>2.532434</td>\n",
       "      <td>2.070910</td>\n",
       "      <td>1.697604</td>\n",
       "      <td>1.617044</td>\n",
       "      <td>1.203158</td>\n",
       "      <td>0.934572</td>\n",
       "      <td>0.643697</td>\n",
       "      <td>0.460003</td>\n",
       "      <td>...</td>\n",
       "      <td>16.637221</td>\n",
       "      <td>17.754412</td>\n",
       "      <td>16.293434</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.025592</td>\n",
       "      <td>-0.010793</td>\n",
       "      <td>-0.022249</td>\n",
       "      <td>-0.011543</td>\n",
       "      <td>20190731_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mfccs0    mfccs1    mfccs2    mfccs3    mfccs4    mfccs5    mfccs6  \\\n",
       "0   5.520031  4.823277  2.519659  1.375790  1.250551  1.212689  0.507529   \n",
       "1   9.383839  8.365168  4.148173  1.992658  1.836000  1.885299  0.919860   \n",
       "2   8.130367  7.069550  3.839147  1.882912  1.592320  1.494450  0.501571   \n",
       "3  10.547326  8.959697  3.605210  1.777991  2.072085  2.201066  0.990699   \n",
       "4   8.259261  7.962046  4.193980  2.526863  1.938031  1.618798  0.947708   \n",
       "5   3.772644  3.019469  1.969844  1.756690  1.475093  1.313248  0.922565   \n",
       "6   5.260175  4.152248  2.736296  2.672591  2.143484  1.869599  1.326102   \n",
       "7   4.875129  4.165122  2.616739  2.305677  1.956817  1.802361  1.360197   \n",
       "8   4.667109  4.149654  2.780008  2.448769  1.989738  1.848565  1.424531   \n",
       "9   3.967470  3.745574  2.532434  2.070910  1.697604  1.617044  1.203158   \n",
       "\n",
       "     mfccs7    mfccs8    mfccs9  ...  contrast4  contrast5  contrast6  \\\n",
       "0  0.363239  0.463409  0.474395  ...  16.018960  17.639429  15.605247   \n",
       "1  0.648280  0.614450  0.432805  ...  17.178497  20.117435  15.817274   \n",
       "2  0.352009  0.417230  0.490138  ...  17.335025  19.589634  15.808904   \n",
       "3  0.778654  1.064813  1.097703  ...  17.349606  21.141986  16.211789   \n",
       "4  0.770062  0.732871  0.404933  ...  17.420084  19.161898  15.782131   \n",
       "5  0.946543  0.689499  0.482501  ...  16.633047  17.794529  16.157175   \n",
       "6  1.226196  0.776981  0.426033  ...  16.435273  17.839591  16.444682   \n",
       "7  1.336472  0.955871  0.582678  ...  16.631618  18.058242  16.392727   \n",
       "8  1.212157  0.929775  0.670839  ...  16.803352  18.052364  16.445027   \n",
       "9  0.934572  0.643697  0.460003  ...  16.637221  17.754412  16.293434   \n",
       "\n",
       "   tonnetz0  tonnetz1  tonnetz2  tonnetz3  tonnetz4  tonnetz5        item  \n",
       "0 -0.042270 -0.014227 -0.020699 -0.038659 -0.022382 -0.003938  20110622_1  \n",
       "1 -0.011430  0.011601 -0.017634 -0.051248  0.004756 -0.004221  20110622_2  \n",
       "2 -0.025843 -0.006595  0.001303  0.011228 -0.010676 -0.012452  20110622_3  \n",
       "3 -0.020132 -0.004959 -0.000051 -0.000602 -0.008769 -0.009025  20110622_4  \n",
       "4 -0.018666  0.019875 -0.010611 -0.034765  0.001899 -0.013111  20110622_5  \n",
       "5  0.003591  0.007420  0.027017 -0.023302 -0.014350 -0.008195  20190731_1  \n",
       "6  0.011317 -0.002543  0.031038 -0.016555 -0.013801 -0.007346  20190731_2  \n",
       "7  0.015156  0.004705  0.018532 -0.011342 -0.016616 -0.007998  20190731_3  \n",
       "8  0.008012 -0.002289  0.019714 -0.034944 -0.020906 -0.006386  20190731_4  \n",
       "9 -0.003763  0.007541  0.025592 -0.010793 -0.022249 -0.011543  20190731_5  \n",
       "\n",
       "[10 rows x 194 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1fc85",
   "metadata": {},
   "source": [
    "### (2)模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2dc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, History, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eacd96",
   "metadata": {},
   "source": [
    "保留5种情绪：{'happy': 0, 'ps': 1, 'neutral': 2, 'sad': 3, 'angry': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c8d7d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfccs0</th>\n",
       "      <th>mfccs1</th>\n",
       "      <th>mfccs2</th>\n",
       "      <th>mfccs3</th>\n",
       "      <th>mfccs4</th>\n",
       "      <th>mfccs5</th>\n",
       "      <th>mfccs6</th>\n",
       "      <th>mfccs7</th>\n",
       "      <th>mfccs8</th>\n",
       "      <th>mfccs9</th>\n",
       "      <th>...</th>\n",
       "      <th>contrast4</th>\n",
       "      <th>contrast5</th>\n",
       "      <th>contrast6</th>\n",
       "      <th>tonnetz0</th>\n",
       "      <th>tonnetz1</th>\n",
       "      <th>tonnetz2</th>\n",
       "      <th>tonnetz3</th>\n",
       "      <th>tonnetz4</th>\n",
       "      <th>tonnetz5</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.022527</td>\n",
       "      <td>0.958868</td>\n",
       "      <td>0.494029</td>\n",
       "      <td>0.307909</td>\n",
       "      <td>0.132615</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>-0.053159</td>\n",
       "      <td>-0.073568</td>\n",
       "      <td>-0.080976</td>\n",
       "      <td>-0.041067</td>\n",
       "      <td>...</td>\n",
       "      <td>17.426861</td>\n",
       "      <td>17.076383</td>\n",
       "      <td>15.520328</td>\n",
       "      <td>-0.042179</td>\n",
       "      <td>0.034978</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>-0.042395</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288416</td>\n",
       "      <td>0.311845</td>\n",
       "      <td>0.235072</td>\n",
       "      <td>0.202644</td>\n",
       "      <td>0.187299</td>\n",
       "      <td>0.173715</td>\n",
       "      <td>0.143612</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.114782</td>\n",
       "      <td>0.088920</td>\n",
       "      <td>...</td>\n",
       "      <td>15.573664</td>\n",
       "      <td>15.925498</td>\n",
       "      <td>15.141866</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.015157</td>\n",
       "      <td>-0.063598</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.406408</td>\n",
       "      <td>0.329117</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>0.182976</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.107178</td>\n",
       "      <td>0.062055</td>\n",
       "      <td>0.030202</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>...</td>\n",
       "      <td>17.362159</td>\n",
       "      <td>16.831559</td>\n",
       "      <td>16.501101</td>\n",
       "      <td>-0.055107</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>-0.087559</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.035862</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.474276</td>\n",
       "      <td>1.524645</td>\n",
       "      <td>0.974791</td>\n",
       "      <td>0.732221</td>\n",
       "      <td>0.644528</td>\n",
       "      <td>0.569810</td>\n",
       "      <td>0.358410</td>\n",
       "      <td>0.111121</td>\n",
       "      <td>-0.048382</td>\n",
       "      <td>-0.038311</td>\n",
       "      <td>...</td>\n",
       "      <td>16.342105</td>\n",
       "      <td>17.730312</td>\n",
       "      <td>14.985024</td>\n",
       "      <td>-0.021259</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>-0.040669</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>-0.014577</td>\n",
       "      <td>-0.014232</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.522091</td>\n",
       "      <td>0.463887</td>\n",
       "      <td>0.341587</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>0.180913</td>\n",
       "      <td>0.114630</td>\n",
       "      <td>0.026273</td>\n",
       "      <td>0.023030</td>\n",
       "      <td>-0.018288</td>\n",
       "      <td>-0.055002</td>\n",
       "      <td>...</td>\n",
       "      <td>18.062980</td>\n",
       "      <td>17.388405</td>\n",
       "      <td>16.019585</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>-0.028387</td>\n",
       "      <td>0.082039</td>\n",
       "      <td>0.012937</td>\n",
       "      <td>0.037996</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>8.490258</td>\n",
       "      <td>7.653400</td>\n",
       "      <td>4.493712</td>\n",
       "      <td>2.629095</td>\n",
       "      <td>1.369422</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>-1.275612</td>\n",
       "      <td>-0.488112</td>\n",
       "      <td>-0.736113</td>\n",
       "      <td>-0.736116</td>\n",
       "      <td>...</td>\n",
       "      <td>21.225740</td>\n",
       "      <td>22.143100</td>\n",
       "      <td>15.800582</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>-0.004746</td>\n",
       "      <td>-0.030455</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>0.026051</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>6.096761</td>\n",
       "      <td>5.610900</td>\n",
       "      <td>4.078144</td>\n",
       "      <td>2.305208</td>\n",
       "      <td>0.619494</td>\n",
       "      <td>0.451949</td>\n",
       "      <td>0.301742</td>\n",
       "      <td>0.249073</td>\n",
       "      <td>-0.063706</td>\n",
       "      <td>-0.400599</td>\n",
       "      <td>...</td>\n",
       "      <td>20.431165</td>\n",
       "      <td>19.394712</td>\n",
       "      <td>17.667970</td>\n",
       "      <td>0.070486</td>\n",
       "      <td>0.102015</td>\n",
       "      <td>0.087497</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.043016</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>9.083913</td>\n",
       "      <td>7.782488</td>\n",
       "      <td>5.288871</td>\n",
       "      <td>3.342988</td>\n",
       "      <td>2.207076</td>\n",
       "      <td>1.257422</td>\n",
       "      <td>0.055670</td>\n",
       "      <td>-0.548027</td>\n",
       "      <td>-0.770778</td>\n",
       "      <td>-0.670887</td>\n",
       "      <td>...</td>\n",
       "      <td>17.811781</td>\n",
       "      <td>18.966005</td>\n",
       "      <td>15.217570</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>0.057701</td>\n",
       "      <td>-0.004201</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>4.847407</td>\n",
       "      <td>4.084107</td>\n",
       "      <td>3.695012</td>\n",
       "      <td>2.926731</td>\n",
       "      <td>1.594043</td>\n",
       "      <td>1.437396</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.532656</td>\n",
       "      <td>0.077507</td>\n",
       "      <td>-0.040866</td>\n",
       "      <td>...</td>\n",
       "      <td>19.969279</td>\n",
       "      <td>19.172665</td>\n",
       "      <td>16.683653</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.056708</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>5.359800</td>\n",
       "      <td>5.321207</td>\n",
       "      <td>4.844370</td>\n",
       "      <td>4.106940</td>\n",
       "      <td>3.183647</td>\n",
       "      <td>2.994763</td>\n",
       "      <td>2.367338</td>\n",
       "      <td>2.102175</td>\n",
       "      <td>1.532907</td>\n",
       "      <td>1.082059</td>\n",
       "      <td>...</td>\n",
       "      <td>19.499500</td>\n",
       "      <td>19.602591</td>\n",
       "      <td>17.277334</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.064002</td>\n",
       "      <td>-0.039156</td>\n",
       "      <td>0.026176</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>-0.008780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2863 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mfccs0    mfccs1    mfccs2    mfccs3    mfccs4    mfccs5    mfccs6  \\\n",
       "0     1.022527  0.958868  0.494029  0.307909  0.132615  0.016355 -0.053159   \n",
       "1     0.288416  0.311845  0.235072  0.202644  0.187299  0.173715  0.143612   \n",
       "2     0.406408  0.329117  0.183439  0.182976  0.133141  0.107178  0.062055   \n",
       "3     1.474276  1.524645  0.974791  0.732221  0.644528  0.569810  0.358410   \n",
       "4     0.522091  0.463887  0.341587  0.267636  0.180913  0.114630  0.026273   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4234  8.490258  7.653400  4.493712  2.629095  1.369422 -0.511180 -1.275612   \n",
       "4235  6.096761  5.610900  4.078144  2.305208  0.619494  0.451949  0.301742   \n",
       "4236  9.083913  7.782488  5.288871  3.342988  2.207076  1.257422  0.055670   \n",
       "4237  4.847407  4.084107  3.695012  2.926731  1.594043  1.437396  0.809804   \n",
       "4238  5.359800  5.321207  4.844370  4.106940  3.183647  2.994763  2.367338   \n",
       "\n",
       "        mfccs7    mfccs8    mfccs9  ...  contrast4  contrast5  contrast6  \\\n",
       "0    -0.073568 -0.080976 -0.041067  ...  17.426861  17.076383  15.520328   \n",
       "1     0.137624  0.114782  0.088920  ...  15.573664  15.925498  15.141866   \n",
       "2     0.030202 -0.004246 -0.001801  ...  17.362159  16.831559  16.501101   \n",
       "3     0.111121 -0.048382 -0.038311  ...  16.342105  17.730312  14.985024   \n",
       "4     0.023030 -0.018288 -0.055002  ...  18.062980  17.388405  16.019585   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "4234 -0.488112 -0.736113 -0.736116  ...  21.225740  22.143100  15.800582   \n",
       "4235  0.249073 -0.063706 -0.400599  ...  20.431165  19.394712  17.667970   \n",
       "4236 -0.548027 -0.770778 -0.670887  ...  17.811781  18.966005  15.217570   \n",
       "4237  0.532656  0.077507 -0.040866  ...  19.969279  19.172665  16.683653   \n",
       "4238  2.102175  1.532907  1.082059  ...  19.499500  19.602591  17.277334   \n",
       "\n",
       "      tonnetz0  tonnetz1  tonnetz2  tonnetz3  tonnetz4  tonnetz5  emotion  \n",
       "0    -0.042179  0.034978  0.012448 -0.042395  0.004703 -0.001524        2  \n",
       "1    -0.000143 -0.015157 -0.063598  0.014533  0.012302  0.001000        2  \n",
       "2    -0.055107  0.034173 -0.087559  0.015745  0.035862  0.000829        2  \n",
       "3    -0.021259  0.054719 -0.040669  0.023162 -0.014577 -0.014232        2  \n",
       "4     0.010363  0.023506 -0.028387  0.082039  0.012937  0.037996        2  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "4234  0.046143 -0.004746 -0.030455 -0.014472  0.026051  0.013821        1  \n",
       "4235  0.070486  0.102015  0.087497  0.000688  0.043016 -0.003614        1  \n",
       "4236  0.012104 -0.000034 -0.039569  0.057701 -0.004201  0.004223        1  \n",
       "4237 -0.003896  0.028683  0.056708  0.004761  0.017381 -0.011286        1  \n",
       "4238  0.005944  0.064002 -0.039156  0.026176  0.007878 -0.008780        1  \n",
       "\n",
       "[2863 rows x 194 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/voice/feature.csv', sep='\\t')\n",
    "df = df[~((df['emotion'] == 2) | (df['emotion'] == 6) | (df['emotion'] == 7))]\n",
    "df['emotion'] = df['emotion'].replace({1: 2, 3: 0, 4: 3, 5: 4, 8: 1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "350d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_set):\n",
    "    '''\n",
    "    分割数据集为训练集和测试集，并保持各个情感类别的平衡\n",
    "    \n",
    "    参数:\n",
    "        df (DataFrame): 包含音频特征和情感标签的数据框\n",
    "        train_set (float): 训练集的比例\n",
    "        \n",
    "    返回:\n",
    "        x_train (numpy array): 训练集特征\n",
    "        y_train (numpy array): 训练集标签\n",
    "        x_test (numpy array): 测试集特征\n",
    "        y_test (numpy array): 测试集标签\n",
    "    '''\n",
    "\n",
    "    # Uncomment to drop a feature\n",
    "##    df = df.drop([col for col in df.columns if \"mfccs\" in col], axis=1)\n",
    "##    df = df.drop([col for col in df.columns if \"chroma\" in col], axis=1)\n",
    "##    df = df.drop([col for col in df.columns if \"mel\" in col], axis=1)\n",
    "    df = df.drop([col for col in df.columns if \"contrast\" in col], axis=1)\n",
    "    df = df.drop([col for col in df.columns if \"tonnetz\" in col], axis=1)\n",
    "    # 为什么要 drop 这两个呢？感觉是试出来的\n",
    "\n",
    "    '''\n",
    "    创建平衡的训练样本\n",
    "    '''\n",
    "    \n",
    "    y_df = df['emotion']\n",
    "    count = []\n",
    "    for emotion in df.emotion.unique():\n",
    "        count.append(len(df[df.emotion == emotion]))\n",
    "\n",
    "    min_count = math.floor(min(count) * train_set)#train_set设训练集比例\n",
    "    x_train, x_test = pd.DataFrame(), pd.DataFrame()\n",
    "    y_train, y_test = pd.DataFrame(columns=['emotion']), pd.DataFrame(columns=['emotion'])\n",
    "    for emotion in df.emotion.unique():\n",
    "        temp = df.loc[df.emotion == emotion]\n",
    "        train_temp = temp.sample(n=min_count, random_state=100)\n",
    "        # left df is the \"big\" one, right df is the sub-set for training, keep if data only appear in the former (i.e., testing data)\n",
    "        test_temp = pd.merge(temp, train_temp, how='outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', 1)\n",
    "        x_train = x_train.append(train_temp.drop(['emotion'], axis=1))\n",
    "        y_train = y_train.append(pd.DataFrame(train_temp['emotion']))\n",
    "        x_test = x_test.append(test_temp.drop(['emotion'], axis=1))\n",
    "        y_test = y_test.append(pd.DataFrame(test_temp['emotion']))\n",
    "\n",
    "    print('Training features:{}; Training output:{}; Testing features:{}; Testing output:{}'.format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "    x_train = x_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    x_test = x_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def test_score(y_test, y_pred):\n",
    "    '''\n",
    "    计算模型在测试集上的准确度\n",
    "    \n",
    "    参数:\n",
    "        y_test (numpy array): 测试集的真实标签\n",
    "        y_pred (numpy array): 模型在测试集上的预测标签\n",
    "    \n",
    "    返回:\n",
    "        accuracy (float): 模型的准确度\n",
    "    '''\n",
    "    \n",
    "    y_pred = np.argmax(y_pred)\n",
    "    y_test = [np.argmax(i, out=None, axis=None) for i in y_test]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def conf_matrix(y_test, y_pred):\n",
    "    '''\n",
    "    计算混淆矩阵\n",
    "    \n",
    "    参数:\n",
    "        y_test (numpy array): 测试集的真实标签\n",
    "        y_pred (numpy array): 模型在测试集上的预测标签\n",
    "    \n",
    "    返回:\n",
    "        matrix (numpy array): 混淆矩阵\n",
    "    '''\n",
    "    \n",
    "    y_pred = np.argmax(y_pred)\n",
    "    y_test = [np.argmax(i, out=None, axis=None) for i in y_test]\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdd45af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\application\\Python\\envs\\swq_env_37\\lib\\site-packages\\ipykernel_launcher.py:40: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features:(1980, 180); Training output:(1980, 1); Testing features:(882, 180); Testing output:(882, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(df, train_set=0.8)\n",
    "\n",
    "###one hot coder\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ab44d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "创建神经网络\n",
    "'''\n",
    "# 目标类别的数量\n",
    "target_class = len(df.emotion.unique())\n",
    "# 输入的特征长度\n",
    "input_length = x_train.shape[1]\n",
    "\n",
    "# 调整参数\n",
    "dense_units = 200\n",
    "dropout = 0.3\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "\n",
    "# 定义神经网络模型\n",
    "model = Sequential()\n",
    "model.add(Dense(dense_units, input_dim=input_length))  # 输入层\n",
    "model.add(Dropout(dropout))  # Dropout 层，用于防止过拟合\n",
    "model.add(Dense(dense_units))  # 隐藏层\n",
    "model.add(Dropout(dropout))  # Dropout 层\n",
    "model.add(Dense(dense_units))  # 隐藏层\n",
    "model.add(Dropout(dropout))  # Dropout 层\n",
    "model.add(Dense(target_class, activation='softmax'))  # 输出层，使用 softmax 激活函数\n",
    "model.compile(loss=loss, optimizer=optimizer,\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
    "                       tf.keras.metrics.Precision(),\n",
    "                       tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d90290d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "31/31 [==============================] - 1s 9ms/step - loss: 1.6892 - categorical_accuracy: 0.5475 - precision_2: 0.6659 - recall_2: 0.4369 - val_loss: 1.0558 - val_categorical_accuracy: 0.6995 - val_precision_2: 0.7987 - val_recall_2: 0.5488 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.0314 - categorical_accuracy: 0.6833 - precision_2: 0.7875 - recall_2: 0.5803 - val_loss: 0.9123 - val_categorical_accuracy: 0.7268 - val_precision_2: 0.8400 - val_recall_2: 0.6190 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.9695 - categorical_accuracy: 0.7177 - precision_2: 0.8231 - recall_2: 0.6343 - val_loss: 0.8267 - val_categorical_accuracy: 0.7358 - val_precision_2: 0.8481 - val_recall_2: 0.6395 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.7901 - categorical_accuracy: 0.7308 - precision_2: 0.8243 - recall_2: 0.6515 - val_loss: 0.7510 - val_categorical_accuracy: 0.7642 - val_precision_2: 0.8798 - val_recall_2: 0.6723 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.7808 - categorical_accuracy: 0.7419 - precision_2: 0.8420 - recall_2: 0.6758 - val_loss: 0.6996 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8951 - val_recall_2: 0.6871 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6778 - categorical_accuracy: 0.7500 - precision_2: 0.8465 - recall_2: 0.6798 - val_loss: 0.7343 - val_categorical_accuracy: 0.7664 - val_precision_2: 0.8679 - val_recall_2: 0.6927 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.7408 - categorical_accuracy: 0.7586 - precision_2: 0.8441 - recall_2: 0.6975 - val_loss: 0.7121 - val_categorical_accuracy: 0.7698 - val_precision_2: 0.8789 - val_recall_2: 0.6995 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6718 - categorical_accuracy: 0.7631 - precision_2: 0.8513 - recall_2: 0.6995 - val_loss: 0.8002 - val_categorical_accuracy: 0.7245 - val_precision_2: 0.8118 - val_recall_2: 0.6553 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6948 - categorical_accuracy: 0.7778 - precision_2: 0.8613 - recall_2: 0.7182 - val_loss: 0.6880 - val_categorical_accuracy: 0.7732 - val_precision_2: 0.8773 - val_recall_2: 0.7132 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6338 - categorical_accuracy: 0.7803 - precision_2: 0.8611 - recall_2: 0.7136 - val_loss: 0.7330 - val_categorical_accuracy: 0.7676 - val_precision_2: 0.8771 - val_recall_2: 0.7041 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.6783 - categorical_accuracy: 0.7677 - precision_2: 0.8534 - recall_2: 0.7116 - val_loss: 0.6716 - val_categorical_accuracy: 0.7755 - val_precision_2: 0.8655 - val_recall_2: 0.7222 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6441 - categorical_accuracy: 0.7667 - precision_2: 0.8462 - recall_2: 0.7141 - val_loss: 0.6872 - val_categorical_accuracy: 0.7574 - val_precision_2: 0.8642 - val_recall_2: 0.6780 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6525 - categorical_accuracy: 0.7732 - precision_2: 0.8559 - recall_2: 0.7111 - val_loss: 0.6876 - val_categorical_accuracy: 0.7710 - val_precision_2: 0.8623 - val_recall_2: 0.7029 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6543 - categorical_accuracy: 0.7788 - precision_2: 0.8715 - recall_2: 0.7227 - val_loss: 0.8449 - val_categorical_accuracy: 0.7562 - val_precision_2: 0.8333 - val_recall_2: 0.7143 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6262 - categorical_accuracy: 0.7773 - precision_2: 0.8553 - recall_2: 0.7167 - val_loss: 0.6919 - val_categorical_accuracy: 0.7664 - val_precision_2: 0.8442 - val_recall_2: 0.7188 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5916 - categorical_accuracy: 0.7727 - precision_2: 0.8535 - recall_2: 0.7207 - val_loss: 0.6765 - val_categorical_accuracy: 0.7676 - val_precision_2: 0.8760 - val_recall_2: 0.7132 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6048 - categorical_accuracy: 0.7742 - precision_2: 0.8568 - recall_2: 0.7253 - val_loss: 0.6886 - val_categorical_accuracy: 0.7472 - val_precision_2: 0.8450 - val_recall_2: 0.6859 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6096 - categorical_accuracy: 0.7859 - precision_2: 0.8625 - recall_2: 0.7253 - val_loss: 0.7310 - val_categorical_accuracy: 0.7721 - val_precision_2: 0.8545 - val_recall_2: 0.7256 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5807 - categorical_accuracy: 0.7924 - precision_2: 0.8715 - recall_2: 0.7434 - val_loss: 0.7488 - val_categorical_accuracy: 0.7653 - val_precision_2: 0.8577 - val_recall_2: 0.7177 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.5542 - categorical_accuracy: 0.7854 - precision_2: 0.8650 - recall_2: 0.7343 - val_loss: 0.6475 - val_categorical_accuracy: 0.7732 - val_precision_2: 0.8908 - val_recall_2: 0.7120 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5339 - categorical_accuracy: 0.7985 - precision_2: 0.8792 - recall_2: 0.7348 - val_loss: 0.6902 - val_categorical_accuracy: 0.7574 - val_precision_2: 0.8437 - val_recall_2: 0.7098 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.6199 - categorical_accuracy: 0.7985 - precision_2: 0.8760 - recall_2: 0.7490 - val_loss: 0.6883 - val_categorical_accuracy: 0.7664 - val_precision_2: 0.8652 - val_recall_2: 0.7279 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5453 - categorical_accuracy: 0.7909 - precision_2: 0.8619 - recall_2: 0.7409 - val_loss: 0.6696 - val_categorical_accuracy: 0.7744 - val_precision_2: 0.8783 - val_recall_2: 0.7120 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.5194 - categorical_accuracy: 0.7985 - precision_2: 0.8757 - recall_2: 0.7439 - val_loss: 0.6404 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8581 - val_recall_2: 0.7404 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5230 - categorical_accuracy: 0.8076 - precision_2: 0.8785 - recall_2: 0.7596 - val_loss: 0.6712 - val_categorical_accuracy: 0.7744 - val_precision_2: 0.8629 - val_recall_2: 0.7279 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5771 - categorical_accuracy: 0.7904 - precision_2: 0.8670 - recall_2: 0.7343 - val_loss: 0.7980 - val_categorical_accuracy: 0.7619 - val_precision_2: 0.8479 - val_recall_2: 0.7143 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5898 - categorical_accuracy: 0.7884 - precision_2: 0.8608 - recall_2: 0.7338 - val_loss: 0.6630 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8641 - val_recall_2: 0.7211 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5306 - categorical_accuracy: 0.7995 - precision_2: 0.8733 - recall_2: 0.7556 - val_loss: 0.6627 - val_categorical_accuracy: 0.7540 - val_precision_2: 0.8356 - val_recall_2: 0.6973 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5522 - categorical_accuracy: 0.7914 - precision_2: 0.8675 - recall_2: 0.7409 - val_loss: 0.6481 - val_categorical_accuracy: 0.7721 - val_precision_2: 0.8554 - val_recall_2: 0.7313 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4870 - categorical_accuracy: 0.8040 - precision_2: 0.8791 - recall_2: 0.7495 - val_loss: 0.6387 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8562 - val_recall_2: 0.7494 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5035 - categorical_accuracy: 0.7980 - precision_2: 0.8727 - recall_2: 0.7510 - val_loss: 0.6802 - val_categorical_accuracy: 0.7438 - val_precision_2: 0.8085 - val_recall_2: 0.7086 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4954 - categorical_accuracy: 0.7955 - precision_2: 0.8712 - recall_2: 0.7551 - val_loss: 0.6336 - val_categorical_accuracy: 0.7766 - val_precision_2: 0.8685 - val_recall_2: 0.7336 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4887 - categorical_accuracy: 0.8076 - precision_2: 0.8846 - recall_2: 0.7551 - val_loss: 0.6202 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8701 - val_recall_2: 0.7290 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4816 - categorical_accuracy: 0.8086 - precision_2: 0.8824 - recall_2: 0.7576 - val_loss: 0.7158 - val_categorical_accuracy: 0.7710 - val_precision_2: 0.8486 - val_recall_2: 0.7245 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4916 - categorical_accuracy: 0.8040 - precision_2: 0.8731 - recall_2: 0.7611 - val_loss: 0.6598 - val_categorical_accuracy: 0.7630 - val_precision_2: 0.8286 - val_recall_2: 0.7234 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5244 - categorical_accuracy: 0.8025 - precision_2: 0.8726 - recall_2: 0.7475 - val_loss: 0.6901 - val_categorical_accuracy: 0.7596 - val_precision_2: 0.8552 - val_recall_2: 0.7098 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5157 - categorical_accuracy: 0.7970 - precision_2: 0.8740 - recall_2: 0.7465 - val_loss: 0.6397 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8795 - val_recall_2: 0.7279 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4636 - categorical_accuracy: 0.8141 - precision_2: 0.8942 - recall_2: 0.7641 - val_loss: 0.6352 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8605 - val_recall_2: 0.7415 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4804 - categorical_accuracy: 0.8101 - precision_2: 0.8904 - recall_2: 0.7591 - val_loss: 0.6696 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8743 - val_recall_2: 0.7256 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4863 - categorical_accuracy: 0.8111 - precision_2: 0.8868 - recall_2: 0.7712 - val_loss: 0.6825 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8662 - val_recall_2: 0.7415 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5008 - categorical_accuracy: 0.8051 - precision_2: 0.8803 - recall_2: 0.7652 - val_loss: 0.6584 - val_categorical_accuracy: 0.7846 - val_precision_2: 0.8654 - val_recall_2: 0.7438 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4925 - categorical_accuracy: 0.7965 - precision_2: 0.8715 - recall_2: 0.7535 - val_loss: 0.6767 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8573 - val_recall_2: 0.7222 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4587 - categorical_accuracy: 0.8152 - precision_2: 0.8906 - recall_2: 0.7732 - val_loss: 0.6532 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8625 - val_recall_2: 0.7324 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4696 - categorical_accuracy: 0.8162 - precision_2: 0.8884 - recall_2: 0.7798 - val_loss: 0.6498 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8731 - val_recall_2: 0.7336 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4485 - categorical_accuracy: 0.8172 - precision_2: 0.8921 - recall_2: 0.7682 - val_loss: 0.6759 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8588 - val_recall_2: 0.7381 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4465 - categorical_accuracy: 0.8207 - precision_2: 0.8857 - recall_2: 0.7828 - val_loss: 0.6681 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8493 - val_recall_2: 0.7347 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4628 - categorical_accuracy: 0.8152 - precision_2: 0.8836 - recall_2: 0.7747 - val_loss: 0.6444 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8820 - val_recall_2: 0.7370 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4582 - categorical_accuracy: 0.8167 - precision_2: 0.8975 - recall_2: 0.7646 - val_loss: 0.6839 - val_categorical_accuracy: 0.7755 - val_precision_2: 0.8482 - val_recall_2: 0.7347 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4665 - categorical_accuracy: 0.8111 - precision_2: 0.8906 - recall_2: 0.7732 - val_loss: 0.6628 - val_categorical_accuracy: 0.7766 - val_precision_2: 0.8475 - val_recall_2: 0.7370 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4551 - categorical_accuracy: 0.8182 - precision_2: 0.8913 - recall_2: 0.7702 - val_loss: 0.6556 - val_categorical_accuracy: 0.7800 - val_precision_2: 0.8624 - val_recall_2: 0.7392 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4393 - categorical_accuracy: 0.8227 - precision_2: 0.8972 - recall_2: 0.7758 - val_loss: 0.6894 - val_categorical_accuracy: 0.7789 - val_precision_2: 0.8361 - val_recall_2: 0.7517 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4555 - categorical_accuracy: 0.8152 - precision_2: 0.8885 - recall_2: 0.7768 - val_loss: 0.6753 - val_categorical_accuracy: 0.7755 - val_precision_2: 0.8644 - val_recall_2: 0.7302 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4636 - categorical_accuracy: 0.8141 - precision_2: 0.8906 - recall_2: 0.7768 - val_loss: 0.7263 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8372 - val_recall_2: 0.7404 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4902 - categorical_accuracy: 0.8136 - precision_2: 0.8759 - recall_2: 0.7768 - val_loss: 0.7661 - val_categorical_accuracy: 0.7789 - val_precision_2: 0.8596 - val_recall_2: 0.7358 - lr: 9.0000e-04\n",
      "Epoch 55/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4684 - categorical_accuracy: 0.8182 - precision_2: 0.8923 - recall_2: 0.7783 - val_loss: 0.7026 - val_categorical_accuracy: 0.7698 - val_precision_2: 0.8289 - val_recall_2: 0.7472 - lr: 9.0000e-04\n",
      "Epoch 56/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4549 - categorical_accuracy: 0.8172 - precision_2: 0.8853 - recall_2: 0.7838 - val_loss: 0.6703 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8568 - val_recall_2: 0.7392 - lr: 9.0000e-04\n",
      "Epoch 57/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4357 - categorical_accuracy: 0.8263 - precision_2: 0.8961 - recall_2: 0.7838 - val_loss: 0.6543 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8547 - val_recall_2: 0.7472 - lr: 9.0000e-04\n",
      "Epoch 58/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4335 - categorical_accuracy: 0.8273 - precision_2: 0.8957 - recall_2: 0.7768 - val_loss: 0.6675 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8551 - val_recall_2: 0.7426 - lr: 9.0000e-04\n",
      "Epoch 59/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4325 - categorical_accuracy: 0.8303 - precision_2: 0.8970 - recall_2: 0.7874 - val_loss: 0.6738 - val_categorical_accuracy: 0.7880 - val_precision_2: 0.8553 - val_recall_2: 0.7506 - lr: 9.0000e-04\n",
      "Epoch 60/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4360 - categorical_accuracy: 0.8384 - precision_2: 0.9103 - recall_2: 0.7889 - val_loss: 0.6650 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8551 - val_recall_2: 0.7426 - lr: 9.0000e-04\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4364 - categorical_accuracy: 0.8247 - precision_2: 0.8905 - recall_2: 0.7843 - val_loss: 0.6761 - val_categorical_accuracy: 0.7721 - val_precision_2: 0.8325 - val_recall_2: 0.7438 - lr: 9.0000e-04\n",
      "Epoch 62/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.5068 - categorical_accuracy: 0.8066 - precision_2: 0.8755 - recall_2: 0.7707 - val_loss: 0.7728 - val_categorical_accuracy: 0.7687 - val_precision_2: 0.8422 - val_recall_2: 0.7324 - lr: 9.0000e-04\n",
      "Epoch 63/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4811 - categorical_accuracy: 0.8096 - precision_2: 0.8799 - recall_2: 0.7732 - val_loss: 0.6727 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8547 - val_recall_2: 0.7404 - lr: 9.0000e-04\n",
      "Epoch 64/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4408 - categorical_accuracy: 0.8283 - precision_2: 0.8980 - recall_2: 0.7960 - val_loss: 0.6968 - val_categorical_accuracy: 0.7755 - val_precision_2: 0.8454 - val_recall_2: 0.7256 - lr: 9.0000e-04\n",
      "Epoch 65/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4352 - categorical_accuracy: 0.8207 - precision_2: 0.8944 - recall_2: 0.7783 - val_loss: 0.6598 - val_categorical_accuracy: 0.7846 - val_precision_2: 0.8443 - val_recall_2: 0.7438 - lr: 9.0000e-04\n",
      "Epoch 66/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4362 - categorical_accuracy: 0.8197 - precision_2: 0.8995 - recall_2: 0.7823 - val_loss: 0.6330 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8717 - val_recall_2: 0.7551 - lr: 9.0000e-04\n",
      "Epoch 67/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4240 - categorical_accuracy: 0.8298 - precision_2: 0.8936 - recall_2: 0.7848 - val_loss: 0.6423 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8697 - val_recall_2: 0.7494 - lr: 9.0000e-04\n",
      "Epoch 68/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4188 - categorical_accuracy: 0.8369 - precision_2: 0.9101 - recall_2: 0.7924 - val_loss: 0.6531 - val_categorical_accuracy: 0.7971 - val_precision_2: 0.8608 - val_recall_2: 0.7574 - lr: 9.0000e-04\n",
      "Epoch 69/200\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4074 - categorical_accuracy: 0.8384 - precision_2: 0.9098 - recall_2: 0.7944 - val_loss: 0.6171 - val_categorical_accuracy: 0.7993 - val_precision_2: 0.8692 - val_recall_2: 0.7608 - lr: 9.0000e-04\n",
      "Epoch 70/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4288 - categorical_accuracy: 0.8333 - precision_2: 0.8944 - recall_2: 0.7955 - val_loss: 0.6930 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8499 - val_recall_2: 0.7381 - lr: 9.0000e-04\n",
      "Epoch 71/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4182 - categorical_accuracy: 0.8273 - precision_2: 0.9047 - recall_2: 0.7818 - val_loss: 0.6844 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8410 - val_recall_2: 0.7494 - lr: 9.0000e-04\n",
      "Epoch 72/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4544 - categorical_accuracy: 0.8247 - precision_2: 0.8968 - recall_2: 0.7944 - val_loss: 0.6426 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8670 - val_recall_2: 0.7540 - lr: 9.0000e-04\n",
      "Epoch 73/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4208 - categorical_accuracy: 0.8308 - precision_2: 0.8956 - recall_2: 0.7843 - val_loss: 0.6838 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8656 - val_recall_2: 0.7449 - lr: 9.0000e-04\n",
      "Epoch 74/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4181 - categorical_accuracy: 0.8298 - precision_2: 0.8954 - recall_2: 0.7869 - val_loss: 0.6695 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8725 - val_recall_2: 0.7449 - lr: 9.0000e-04\n",
      "Epoch 75/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4056 - categorical_accuracy: 0.8374 - precision_2: 0.9098 - recall_2: 0.7995 - val_loss: 0.6653 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8485 - val_recall_2: 0.7494 - lr: 9.0000e-04\n",
      "Epoch 76/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4132 - categorical_accuracy: 0.8359 - precision_2: 0.8999 - recall_2: 0.7904 - val_loss: 0.6992 - val_categorical_accuracy: 0.7766 - val_precision_2: 0.8583 - val_recall_2: 0.7347 - lr: 9.0000e-04\n",
      "Epoch 77/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4153 - categorical_accuracy: 0.8303 - precision_2: 0.9036 - recall_2: 0.7949 - val_loss: 0.6524 - val_categorical_accuracy: 0.8039 - val_precision_2: 0.8661 - val_recall_2: 0.7551 - lr: 9.0000e-04\n",
      "Epoch 78/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4330 - categorical_accuracy: 0.8364 - precision_2: 0.9057 - recall_2: 0.7904 - val_loss: 0.7297 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8505 - val_recall_2: 0.7483 - lr: 9.0000e-04\n",
      "Epoch 79/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4721 - categorical_accuracy: 0.8227 - precision_2: 0.8821 - recall_2: 0.7859 - val_loss: 0.6875 - val_categorical_accuracy: 0.7778 - val_precision_2: 0.8381 - val_recall_2: 0.7336 - lr: 9.0000e-04\n",
      "Epoch 80/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4192 - categorical_accuracy: 0.8318 - precision_2: 0.8951 - recall_2: 0.7889 - val_loss: 0.7002 - val_categorical_accuracy: 0.8039 - val_precision_2: 0.8734 - val_recall_2: 0.7585 - lr: 9.0000e-04\n",
      "Epoch 81/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4297 - categorical_accuracy: 0.8348 - precision_2: 0.9007 - recall_2: 0.7879 - val_loss: 0.6880 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8605 - val_recall_2: 0.7483 - lr: 9.0000e-04\n",
      "Epoch 82/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4286 - categorical_accuracy: 0.8359 - precision_2: 0.9060 - recall_2: 0.7884 - val_loss: 0.6784 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8758 - val_recall_2: 0.7438 - lr: 9.0000e-04\n",
      "Epoch 83/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4208 - categorical_accuracy: 0.8298 - precision_2: 0.9031 - recall_2: 0.7904 - val_loss: 0.6765 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8544 - val_recall_2: 0.7449 - lr: 9.0000e-04\n",
      "Epoch 84/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4275 - categorical_accuracy: 0.8288 - precision_2: 0.9028 - recall_2: 0.7884 - val_loss: 0.7604 - val_categorical_accuracy: 0.7868 - val_precision_2: 0.8426 - val_recall_2: 0.7585 - lr: 9.0000e-04\n",
      "Epoch 85/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4341 - categorical_accuracy: 0.8313 - precision_2: 0.9020 - recall_2: 0.7949 - val_loss: 0.7791 - val_categorical_accuracy: 0.7676 - val_precision_2: 0.8452 - val_recall_2: 0.7245 - lr: 9.0000e-04\n",
      "Epoch 86/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4274 - categorical_accuracy: 0.8283 - precision_2: 0.8958 - recall_2: 0.7899 - val_loss: 0.7119 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8499 - val_recall_2: 0.7381 - lr: 9.0000e-04\n",
      "Epoch 87/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4182 - categorical_accuracy: 0.8369 - precision_2: 0.8981 - recall_2: 0.7965 - val_loss: 0.7033 - val_categorical_accuracy: 0.7800 - val_precision_2: 0.8613 - val_recall_2: 0.7392 - lr: 9.0000e-04\n",
      "Epoch 88/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4133 - categorical_accuracy: 0.8328 - precision_2: 0.9066 - recall_2: 0.7894 - val_loss: 0.7355 - val_categorical_accuracy: 0.7721 - val_precision_2: 0.8241 - val_recall_2: 0.7438 - lr: 9.0000e-04\n",
      "Epoch 89/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4116 - categorical_accuracy: 0.8318 - precision_2: 0.8963 - recall_2: 0.7985 - val_loss: 0.6751 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8527 - val_recall_2: 0.7483 - lr: 9.0000e-04\n",
      "Epoch 90/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3913 - categorical_accuracy: 0.8374 - precision_2: 0.9039 - recall_2: 0.7980 - val_loss: 0.6944 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8442 - val_recall_2: 0.7494 - lr: 8.1000e-04\n",
      "Epoch 91/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3970 - categorical_accuracy: 0.8414 - precision_2: 0.9081 - recall_2: 0.7985 - val_loss: 0.6912 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8520 - val_recall_2: 0.7574 - lr: 8.1000e-04\n",
      "Epoch 92/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4050 - categorical_accuracy: 0.8379 - precision_2: 0.8993 - recall_2: 0.7939 - val_loss: 0.6733 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8670 - val_recall_2: 0.7540 - lr: 8.1000e-04\n",
      "Epoch 93/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4128 - categorical_accuracy: 0.8364 - precision_2: 0.9033 - recall_2: 0.7975 - val_loss: 0.6538 - val_categorical_accuracy: 0.8039 - val_precision_2: 0.8651 - val_recall_2: 0.7562 - lr: 8.1000e-04\n",
      "Epoch 94/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4040 - categorical_accuracy: 0.8389 - precision_2: 0.9031 - recall_2: 0.8000 - val_loss: 0.6619 - val_categorical_accuracy: 0.8027 - val_precision_2: 0.8703 - val_recall_2: 0.7608 - lr: 8.1000e-04\n",
      "Epoch 95/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4145 - categorical_accuracy: 0.8374 - precision_2: 0.8995 - recall_2: 0.7955 - val_loss: 0.7016 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8477 - val_recall_2: 0.7449 - lr: 8.1000e-04\n",
      "Epoch 96/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4178 - categorical_accuracy: 0.8333 - precision_2: 0.9051 - recall_2: 0.7995 - val_loss: 0.7013 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8519 - val_recall_2: 0.7438 - lr: 8.1000e-04\n",
      "Epoch 97/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4158 - categorical_accuracy: 0.8359 - precision_2: 0.8989 - recall_2: 0.7899 - val_loss: 0.6962 - val_categorical_accuracy: 0.7880 - val_precision_2: 0.8501 - val_recall_2: 0.7585 - lr: 8.1000e-04\n",
      "Epoch 98/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4206 - categorical_accuracy: 0.8283 - precision_2: 0.8924 - recall_2: 0.7955 - val_loss: 0.7123 - val_categorical_accuracy: 0.7880 - val_precision_2: 0.8474 - val_recall_2: 0.7494 - lr: 8.1000e-04\n",
      "Epoch 99/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4590 - categorical_accuracy: 0.8263 - precision_2: 0.9002 - recall_2: 0.7884 - val_loss: 0.7019 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8531 - val_recall_2: 0.7506 - lr: 8.1000e-04\n",
      "Epoch 100/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4495 - categorical_accuracy: 0.8222 - precision_2: 0.8883 - recall_2: 0.7869 - val_loss: 0.6756 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8506 - val_recall_2: 0.7551 - lr: 8.1000e-04\n",
      "Epoch 101/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4270 - categorical_accuracy: 0.8409 - precision_2: 0.9056 - recall_2: 0.8040 - val_loss: 0.6766 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8717 - val_recall_2: 0.7472 - lr: 8.1000e-04\n",
      "Epoch 102/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4164 - categorical_accuracy: 0.8318 - precision_2: 0.9026 - recall_2: 0.7914 - val_loss: 0.6802 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8602 - val_recall_2: 0.7392 - lr: 8.1000e-04\n",
      "Epoch 103/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3942 - categorical_accuracy: 0.8455 - precision_2: 0.9170 - recall_2: 0.8030 - val_loss: 0.6626 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8590 - val_recall_2: 0.7528 - lr: 8.1000e-04\n",
      "Epoch 104/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3903 - categorical_accuracy: 0.8465 - precision_2: 0.9108 - recall_2: 0.8045 - val_loss: 0.7093 - val_categorical_accuracy: 0.8016 - val_precision_2: 0.8711 - val_recall_2: 0.7585 - lr: 8.1000e-04\n",
      "Epoch 105/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4130 - categorical_accuracy: 0.8444 - precision_2: 0.9085 - recall_2: 0.8025 - val_loss: 0.7284 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8606 - val_recall_2: 0.7562 - lr: 8.1000e-04\n",
      "Epoch 106/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4160 - categorical_accuracy: 0.8253 - precision_2: 0.8950 - recall_2: 0.7838 - val_loss: 0.6978 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8614 - val_recall_2: 0.7540 - lr: 8.1000e-04\n",
      "Epoch 107/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3985 - categorical_accuracy: 0.8449 - precision_2: 0.9131 - recall_2: 0.8015 - val_loss: 0.7292 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8480 - val_recall_2: 0.7528 - lr: 8.1000e-04\n",
      "Epoch 108/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4047 - categorical_accuracy: 0.8374 - precision_2: 0.8961 - recall_2: 0.8015 - val_loss: 0.7069 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8630 - val_recall_2: 0.7426 - lr: 8.1000e-04\n",
      "Epoch 109/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3820 - categorical_accuracy: 0.8515 - precision_2: 0.9157 - recall_2: 0.8061 - val_loss: 0.6656 - val_categorical_accuracy: 0.7880 - val_precision_2: 0.8558 - val_recall_2: 0.7472 - lr: 8.1000e-04\n",
      "Epoch 110/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4056 - categorical_accuracy: 0.8424 - precision_2: 0.9030 - recall_2: 0.8035 - val_loss: 0.6730 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8522 - val_recall_2: 0.7517 - lr: 7.2900e-04\n",
      "Epoch 111/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3947 - categorical_accuracy: 0.8434 - precision_2: 0.9091 - recall_2: 0.8035 - val_loss: 0.7206 - val_categorical_accuracy: 0.7744 - val_precision_2: 0.8346 - val_recall_2: 0.7381 - lr: 7.2900e-04\n",
      "Epoch 112/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3787 - categorical_accuracy: 0.8444 - precision_2: 0.9120 - recall_2: 0.8010 - val_loss: 0.7148 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8452 - val_recall_2: 0.7551 - lr: 7.2900e-04\n",
      "Epoch 113/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4087 - categorical_accuracy: 0.8384 - precision_2: 0.8983 - recall_2: 0.8030 - val_loss: 0.7408 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8424 - val_recall_2: 0.7574 - lr: 7.2900e-04\n",
      "Epoch 114/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3992 - categorical_accuracy: 0.8394 - precision_2: 0.9027 - recall_2: 0.8010 - val_loss: 0.6681 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8582 - val_recall_2: 0.7551 - lr: 7.2900e-04\n",
      "Epoch 115/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3910 - categorical_accuracy: 0.8480 - precision_2: 0.9123 - recall_2: 0.8040 - val_loss: 0.7060 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8436 - val_recall_2: 0.7460 - lr: 7.2900e-04\n",
      "Epoch 116/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4293 - categorical_accuracy: 0.8409 - precision_2: 0.9000 - recall_2: 0.8045 - val_loss: 0.7642 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8333 - val_recall_2: 0.7540 - lr: 7.2900e-04\n",
      "Epoch 117/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4590 - categorical_accuracy: 0.8293 - precision_2: 0.8839 - recall_2: 0.7995 - val_loss: 0.6961 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8537 - val_recall_2: 0.7608 - lr: 7.2900e-04\n",
      "Epoch 118/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4161 - categorical_accuracy: 0.8399 - precision_2: 0.8990 - recall_2: 0.8005 - val_loss: 0.7665 - val_categorical_accuracy: 0.7732 - val_precision_2: 0.8408 - val_recall_2: 0.7426 - lr: 7.2900e-04\n",
      "Epoch 119/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4089 - categorical_accuracy: 0.8419 - precision_2: 0.9026 - recall_2: 0.7955 - val_loss: 0.6887 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8553 - val_recall_2: 0.7642 - lr: 7.2900e-04\n",
      "Epoch 120/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3967 - categorical_accuracy: 0.8455 - precision_2: 0.9032 - recall_2: 0.8061 - val_loss: 0.7088 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8540 - val_recall_2: 0.7562 - lr: 7.2900e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3908 - categorical_accuracy: 0.8455 - precision_2: 0.9123 - recall_2: 0.8086 - val_loss: 0.7485 - val_categorical_accuracy: 0.7823 - val_precision_2: 0.8346 - val_recall_2: 0.7551 - lr: 7.2900e-04\n",
      "Epoch 122/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3993 - categorical_accuracy: 0.8369 - precision_2: 0.9004 - recall_2: 0.7990 - val_loss: 0.7874 - val_categorical_accuracy: 0.7732 - val_precision_2: 0.8406 - val_recall_2: 0.7472 - lr: 7.2900e-04\n",
      "Epoch 123/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4103 - categorical_accuracy: 0.8419 - precision_2: 0.9027 - recall_2: 0.8010 - val_loss: 0.7316 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8465 - val_recall_2: 0.7506 - lr: 7.2900e-04\n",
      "Epoch 124/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3763 - categorical_accuracy: 0.8490 - precision_2: 0.9143 - recall_2: 0.8081 - val_loss: 0.6984 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8519 - val_recall_2: 0.7630 - lr: 7.2900e-04\n",
      "Epoch 125/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3749 - categorical_accuracy: 0.8510 - precision_2: 0.9158 - recall_2: 0.8131 - val_loss: 0.7070 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8627 - val_recall_2: 0.7483 - lr: 7.2900e-04\n",
      "Epoch 126/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3804 - categorical_accuracy: 0.8455 - precision_2: 0.9065 - recall_2: 0.8076 - val_loss: 0.6881 - val_categorical_accuracy: 0.8050 - val_precision_2: 0.8602 - val_recall_2: 0.7676 - lr: 7.2900e-04\n",
      "Epoch 127/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4224 - categorical_accuracy: 0.8455 - precision_2: 0.9054 - recall_2: 0.8025 - val_loss: 0.7672 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8298 - val_recall_2: 0.7574 - lr: 7.2900e-04\n",
      "Epoch 128/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4004 - categorical_accuracy: 0.8409 - precision_2: 0.8996 - recall_2: 0.8096 - val_loss: 0.7314 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8570 - val_recall_2: 0.7472 - lr: 7.2900e-04\n",
      "Epoch 129/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3815 - categorical_accuracy: 0.8530 - precision_2: 0.9126 - recall_2: 0.8121 - val_loss: 0.7108 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8577 - val_recall_2: 0.7585 - lr: 7.2900e-04\n",
      "Epoch 130/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3697 - categorical_accuracy: 0.8485 - precision_2: 0.9113 - recall_2: 0.8146 - val_loss: 0.6935 - val_categorical_accuracy: 0.8027 - val_precision_2: 0.8625 - val_recall_2: 0.7608 - lr: 6.5610e-04\n",
      "Epoch 131/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3816 - categorical_accuracy: 0.8444 - precision_2: 0.9075 - recall_2: 0.8076 - val_loss: 0.7034 - val_categorical_accuracy: 0.7971 - val_precision_2: 0.8573 - val_recall_2: 0.7562 - lr: 6.5610e-04\n",
      "Epoch 132/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3903 - categorical_accuracy: 0.8495 - precision_2: 0.9043 - recall_2: 0.8157 - val_loss: 0.7200 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8649 - val_recall_2: 0.7551 - lr: 6.5610e-04\n",
      "Epoch 133/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3730 - categorical_accuracy: 0.8470 - precision_2: 0.9100 - recall_2: 0.8116 - val_loss: 0.7211 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8521 - val_recall_2: 0.7642 - lr: 6.5610e-04\n",
      "Epoch 134/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3706 - categorical_accuracy: 0.8571 - precision_2: 0.9134 - recall_2: 0.8152 - val_loss: 0.7617 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8466 - val_recall_2: 0.7449 - lr: 6.5610e-04\n",
      "Epoch 135/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3846 - categorical_accuracy: 0.8439 - precision_2: 0.9036 - recall_2: 0.8000 - val_loss: 0.7287 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8729 - val_recall_2: 0.7630 - lr: 6.5610e-04\n",
      "Epoch 136/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3821 - categorical_accuracy: 0.8525 - precision_2: 0.9098 - recall_2: 0.8146 - val_loss: 0.7290 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8501 - val_recall_2: 0.7585 - lr: 6.5610e-04\n",
      "Epoch 137/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3798 - categorical_accuracy: 0.8404 - precision_2: 0.9056 - recall_2: 0.8096 - val_loss: 0.7204 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8530 - val_recall_2: 0.7630 - lr: 6.5610e-04\n",
      "Epoch 138/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3730 - categorical_accuracy: 0.8540 - precision_2: 0.9145 - recall_2: 0.8106 - val_loss: 0.7263 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8533 - val_recall_2: 0.7585 - lr: 6.5610e-04\n",
      "Epoch 139/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3693 - categorical_accuracy: 0.8540 - precision_2: 0.9098 - recall_2: 0.8152 - val_loss: 0.7141 - val_categorical_accuracy: 0.7993 - val_precision_2: 0.8627 - val_recall_2: 0.7551 - lr: 6.5610e-04\n",
      "Epoch 140/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3681 - categorical_accuracy: 0.8530 - precision_2: 0.9094 - recall_2: 0.8167 - val_loss: 0.7480 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8473 - val_recall_2: 0.7551 - lr: 6.5610e-04\n",
      "Epoch 141/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3841 - categorical_accuracy: 0.8485 - precision_2: 0.9105 - recall_2: 0.8121 - val_loss: 0.6907 - val_categorical_accuracy: 0.8016 - val_precision_2: 0.8628 - val_recall_2: 0.7630 - lr: 6.5610e-04\n",
      "Epoch 142/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3943 - categorical_accuracy: 0.8333 - precision_2: 0.8896 - recall_2: 0.7980 - val_loss: 0.7086 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8628 - val_recall_2: 0.7415 - lr: 6.5610e-04\n",
      "Epoch 143/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4085 - categorical_accuracy: 0.8444 - precision_2: 0.9069 - recall_2: 0.8015 - val_loss: 0.7794 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8501 - val_recall_2: 0.7585 - lr: 6.5610e-04\n",
      "Epoch 144/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4224 - categorical_accuracy: 0.8379 - precision_2: 0.8975 - recall_2: 0.8005 - val_loss: 0.7412 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8542 - val_recall_2: 0.7506 - lr: 6.5610e-04\n",
      "Epoch 145/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3762 - categorical_accuracy: 0.8505 - precision_2: 0.9150 - recall_2: 0.8152 - val_loss: 0.7131 - val_categorical_accuracy: 0.7971 - val_precision_2: 0.8505 - val_recall_2: 0.7483 - lr: 6.5610e-04\n",
      "Epoch 146/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4301 - categorical_accuracy: 0.8384 - precision_2: 0.8938 - recall_2: 0.8035 - val_loss: 0.8450 - val_categorical_accuracy: 0.7868 - val_precision_2: 0.8321 - val_recall_2: 0.7472 - lr: 6.5610e-04\n",
      "Epoch 147/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3898 - categorical_accuracy: 0.8429 - precision_2: 0.9066 - recall_2: 0.8091 - val_loss: 0.7667 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8537 - val_recall_2: 0.7540 - lr: 6.5610e-04\n",
      "Epoch 148/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3826 - categorical_accuracy: 0.8500 - precision_2: 0.9093 - recall_2: 0.8101 - val_loss: 0.7018 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8573 - val_recall_2: 0.7698 - lr: 6.5610e-04\n",
      "Epoch 149/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3740 - categorical_accuracy: 0.8540 - precision_2: 0.9136 - recall_2: 0.8116 - val_loss: 0.7325 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8513 - val_recall_2: 0.7528 - lr: 6.5610e-04\n",
      "Epoch 150/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3756 - categorical_accuracy: 0.8556 - precision_2: 0.9121 - recall_2: 0.8177 - val_loss: 0.7629 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8352 - val_recall_2: 0.7585 - lr: 5.9049e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3831 - categorical_accuracy: 0.8485 - precision_2: 0.9062 - recall_2: 0.8101 - val_loss: 0.7462 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8612 - val_recall_2: 0.7596 - lr: 5.9049e-04\n",
      "Epoch 152/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3664 - categorical_accuracy: 0.8535 - precision_2: 0.9175 - recall_2: 0.8141 - val_loss: 0.7323 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8457 - val_recall_2: 0.7642 - lr: 5.9049e-04\n",
      "Epoch 153/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3965 - categorical_accuracy: 0.8409 - precision_2: 0.9010 - recall_2: 0.8086 - val_loss: 0.7358 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8586 - val_recall_2: 0.7642 - lr: 5.9049e-04\n",
      "Epoch 154/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3717 - categorical_accuracy: 0.8525 - precision_2: 0.9108 - recall_2: 0.8202 - val_loss: 0.7534 - val_categorical_accuracy: 0.7834 - val_precision_2: 0.8674 - val_recall_2: 0.7415 - lr: 5.9049e-04\n",
      "Epoch 155/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3625 - categorical_accuracy: 0.8641 - precision_2: 0.9238 - recall_2: 0.8263 - val_loss: 0.7307 - val_categorical_accuracy: 0.8016 - val_precision_2: 0.8568 - val_recall_2: 0.7664 - lr: 5.9049e-04\n",
      "Epoch 156/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3623 - categorical_accuracy: 0.8596 - precision_2: 0.9127 - recall_2: 0.8232 - val_loss: 0.7067 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8481 - val_recall_2: 0.7596 - lr: 5.9049e-04\n",
      "Epoch 157/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3669 - categorical_accuracy: 0.8465 - precision_2: 0.9045 - recall_2: 0.8086 - val_loss: 0.7128 - val_categorical_accuracy: 0.8016 - val_precision_2: 0.8530 - val_recall_2: 0.7630 - lr: 5.9049e-04\n",
      "Epoch 158/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3560 - categorical_accuracy: 0.8591 - precision_2: 0.9184 - recall_2: 0.8247 - val_loss: 0.7671 - val_categorical_accuracy: 0.7800 - val_precision_2: 0.8449 - val_recall_2: 0.7472 - lr: 5.9049e-04\n",
      "Epoch 159/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3574 - categorical_accuracy: 0.8566 - precision_2: 0.9132 - recall_2: 0.8182 - val_loss: 0.7443 - val_categorical_accuracy: 0.7868 - val_precision_2: 0.8526 - val_recall_2: 0.7540 - lr: 5.9049e-04\n",
      "Epoch 160/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3715 - categorical_accuracy: 0.8475 - precision_2: 0.9033 - recall_2: 0.8111 - val_loss: 0.7502 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8419 - val_recall_2: 0.7608 - lr: 5.9049e-04\n",
      "Epoch 161/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3818 - categorical_accuracy: 0.8490 - precision_2: 0.9091 - recall_2: 0.8131 - val_loss: 0.7390 - val_categorical_accuracy: 0.7937 - val_precision_2: 0.8436 - val_recall_2: 0.7642 - lr: 5.9049e-04\n",
      "Epoch 162/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3465 - categorical_accuracy: 0.8611 - precision_2: 0.9143 - recall_2: 0.8247 - val_loss: 0.7150 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8674 - val_recall_2: 0.7562 - lr: 5.9049e-04\n",
      "Epoch 163/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3463 - categorical_accuracy: 0.8530 - precision_2: 0.9163 - recall_2: 0.8187 - val_loss: 0.7256 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8601 - val_recall_2: 0.7528 - lr: 5.9049e-04\n",
      "Epoch 164/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3609 - categorical_accuracy: 0.8596 - precision_2: 0.9156 - recall_2: 0.8167 - val_loss: 0.7260 - val_categorical_accuracy: 0.8027 - val_precision_2: 0.8577 - val_recall_2: 0.7721 - lr: 5.9049e-04\n",
      "Epoch 165/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3591 - categorical_accuracy: 0.8581 - precision_2: 0.9123 - recall_2: 0.8192 - val_loss: 0.7241 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8501 - val_recall_2: 0.7585 - lr: 5.9049e-04\n",
      "Epoch 166/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3722 - categorical_accuracy: 0.8545 - precision_2: 0.9107 - recall_2: 0.8187 - val_loss: 0.7209 - val_categorical_accuracy: 0.8050 - val_precision_2: 0.8611 - val_recall_2: 0.7664 - lr: 5.9049e-04\n",
      "Epoch 167/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3519 - categorical_accuracy: 0.8540 - precision_2: 0.9105 - recall_2: 0.8172 - val_loss: 0.7627 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8525 - val_recall_2: 0.7664 - lr: 5.9049e-04\n",
      "Epoch 168/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3634 - categorical_accuracy: 0.8591 - precision_2: 0.9150 - recall_2: 0.8212 - val_loss: 0.7163 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8528 - val_recall_2: 0.7619 - lr: 5.9049e-04\n",
      "Epoch 169/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3754 - categorical_accuracy: 0.8510 - precision_2: 0.9042 - recall_2: 0.8152 - val_loss: 0.7744 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8562 - val_recall_2: 0.7630 - lr: 5.9049e-04\n",
      "Epoch 170/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3608 - categorical_accuracy: 0.8505 - precision_2: 0.9108 - recall_2: 0.8146 - val_loss: 0.7958 - val_categorical_accuracy: 0.7789 - val_precision_2: 0.8367 - val_recall_2: 0.7494 - lr: 5.3144e-04\n",
      "Epoch 171/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3715 - categorical_accuracy: 0.8525 - precision_2: 0.9087 - recall_2: 0.8146 - val_loss: 0.7059 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8539 - val_recall_2: 0.7687 - lr: 5.3144e-04\n",
      "Epoch 172/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3585 - categorical_accuracy: 0.8480 - precision_2: 0.9072 - recall_2: 0.8192 - val_loss: 0.7714 - val_categorical_accuracy: 0.7880 - val_precision_2: 0.8506 - val_recall_2: 0.7619 - lr: 5.3144e-04\n",
      "Epoch 173/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3633 - categorical_accuracy: 0.8561 - precision_2: 0.9175 - recall_2: 0.8197 - val_loss: 0.7942 - val_categorical_accuracy: 0.7925 - val_precision_2: 0.8519 - val_recall_2: 0.7562 - lr: 5.3144e-04\n",
      "Epoch 174/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3586 - categorical_accuracy: 0.8535 - precision_2: 0.9119 - recall_2: 0.8157 - val_loss: 0.8406 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8359 - val_recall_2: 0.7506 - lr: 5.3144e-04\n",
      "Epoch 175/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3813 - categorical_accuracy: 0.8490 - precision_2: 0.9054 - recall_2: 0.8121 - val_loss: 0.7907 - val_categorical_accuracy: 0.7868 - val_precision_2: 0.8352 - val_recall_2: 0.7642 - lr: 5.3144e-04\n",
      "Epoch 176/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3822 - categorical_accuracy: 0.8556 - precision_2: 0.9062 - recall_2: 0.8152 - val_loss: 0.7454 - val_categorical_accuracy: 0.7993 - val_precision_2: 0.8557 - val_recall_2: 0.7596 - lr: 5.3144e-04\n",
      "Epoch 177/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3742 - categorical_accuracy: 0.8520 - precision_2: 0.9074 - recall_2: 0.8116 - val_loss: 0.7857 - val_categorical_accuracy: 0.7857 - val_precision_2: 0.8467 - val_recall_2: 0.7517 - lr: 5.3144e-04\n",
      "Epoch 178/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3736 - categorical_accuracy: 0.8460 - precision_2: 0.9045 - recall_2: 0.8136 - val_loss: 0.7452 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8460 - val_recall_2: 0.7721 - lr: 5.3144e-04\n",
      "Epoch 179/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3661 - categorical_accuracy: 0.8545 - precision_2: 0.9152 - recall_2: 0.8177 - val_loss: 0.7248 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8582 - val_recall_2: 0.7687 - lr: 5.3144e-04\n",
      "Epoch 180/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3632 - categorical_accuracy: 0.8551 - precision_2: 0.9091 - recall_2: 0.8182 - val_loss: 0.7559 - val_categorical_accuracy: 0.7902 - val_precision_2: 0.8506 - val_recall_2: 0.7551 - lr: 5.3144e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3835 - categorical_accuracy: 0.8586 - precision_2: 0.9110 - recall_2: 0.8167 - val_loss: 0.8655 - val_categorical_accuracy: 0.7755 - val_precision_2: 0.8242 - val_recall_2: 0.7494 - lr: 5.3144e-04\n",
      "Epoch 182/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4157 - categorical_accuracy: 0.8429 - precision_2: 0.9017 - recall_2: 0.8061 - val_loss: 0.7916 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8395 - val_recall_2: 0.7710 - lr: 5.3144e-04\n",
      "Epoch 183/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3686 - categorical_accuracy: 0.8530 - precision_2: 0.9120 - recall_2: 0.8162 - val_loss: 0.7899 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8539 - val_recall_2: 0.7619 - lr: 5.3144e-04\n",
      "Epoch 184/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3671 - categorical_accuracy: 0.8561 - precision_2: 0.9101 - recall_2: 0.8182 - val_loss: 0.7710 - val_categorical_accuracy: 0.7971 - val_precision_2: 0.8464 - val_recall_2: 0.7687 - lr: 5.3144e-04\n",
      "Epoch 185/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3618 - categorical_accuracy: 0.8556 - precision_2: 0.9174 - recall_2: 0.8192 - val_loss: 0.7683 - val_categorical_accuracy: 0.7959 - val_precision_2: 0.8493 - val_recall_2: 0.7540 - lr: 5.3144e-04\n",
      "Epoch 186/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3640 - categorical_accuracy: 0.8510 - precision_2: 0.9102 - recall_2: 0.8141 - val_loss: 0.7830 - val_categorical_accuracy: 0.7812 - val_precision_2: 0.8378 - val_recall_2: 0.7438 - lr: 5.3144e-04\n",
      "Epoch 187/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3547 - categorical_accuracy: 0.8490 - precision_2: 0.9138 - recall_2: 0.8136 - val_loss: 0.8221 - val_categorical_accuracy: 0.7766 - val_precision_2: 0.8269 - val_recall_2: 0.7528 - lr: 5.3144e-04\n",
      "Epoch 188/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3695 - categorical_accuracy: 0.8561 - precision_2: 0.8970 - recall_2: 0.8227 - val_loss: 0.7725 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8423 - val_recall_2: 0.7630 - lr: 5.3144e-04\n",
      "Epoch 189/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3569 - categorical_accuracy: 0.8545 - precision_2: 0.9111 - recall_2: 0.8182 - val_loss: 0.7623 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8532 - val_recall_2: 0.7642 - lr: 5.3144e-04\n",
      "Epoch 190/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3464 - categorical_accuracy: 0.8576 - precision_2: 0.9126 - recall_2: 0.8222 - val_loss: 0.7348 - val_categorical_accuracy: 0.8016 - val_precision_2: 0.8647 - val_recall_2: 0.7608 - lr: 4.7830e-04\n",
      "Epoch 191/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3431 - categorical_accuracy: 0.8657 - precision_2: 0.9260 - recall_2: 0.8278 - val_loss: 0.7987 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8506 - val_recall_2: 0.7551 - lr: 4.7830e-04\n",
      "Epoch 192/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3535 - categorical_accuracy: 0.8636 - precision_2: 0.9159 - recall_2: 0.8247 - val_loss: 0.7744 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8505 - val_recall_2: 0.7676 - lr: 4.7830e-04\n",
      "Epoch 193/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3499 - categorical_accuracy: 0.8616 - precision_2: 0.9140 - recall_2: 0.8268 - val_loss: 0.7476 - val_categorical_accuracy: 0.7914 - val_precision_2: 0.8562 - val_recall_2: 0.7562 - lr: 4.7830e-04\n",
      "Epoch 194/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3546 - categorical_accuracy: 0.8596 - precision_2: 0.9133 - recall_2: 0.8197 - val_loss: 0.7609 - val_categorical_accuracy: 0.8005 - val_precision_2: 0.8580 - val_recall_2: 0.7676 - lr: 4.7830e-04\n",
      "Epoch 195/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3570 - categorical_accuracy: 0.8591 - precision_2: 0.9084 - recall_2: 0.8217 - val_loss: 0.7846 - val_categorical_accuracy: 0.7846 - val_precision_2: 0.8433 - val_recall_2: 0.7506 - lr: 4.7830e-04\n",
      "Epoch 196/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3473 - categorical_accuracy: 0.8646 - precision_2: 0.9222 - recall_2: 0.8258 - val_loss: 0.7658 - val_categorical_accuracy: 0.7891 - val_precision_2: 0.8447 - val_recall_2: 0.7710 - lr: 4.7830e-04\n",
      "Epoch 197/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3329 - categorical_accuracy: 0.8626 - precision_2: 0.9230 - recall_2: 0.8293 - val_loss: 0.7619 - val_categorical_accuracy: 0.7982 - val_precision_2: 0.8573 - val_recall_2: 0.7630 - lr: 4.7830e-04\n",
      "Epoch 198/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3446 - categorical_accuracy: 0.8652 - precision_2: 0.9166 - recall_2: 0.8328 - val_loss: 0.7748 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8555 - val_recall_2: 0.7585 - lr: 4.7830e-04\n",
      "Epoch 199/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3568 - categorical_accuracy: 0.8525 - precision_2: 0.9131 - recall_2: 0.8227 - val_loss: 0.7647 - val_categorical_accuracy: 0.7971 - val_precision_2: 0.8579 - val_recall_2: 0.7664 - lr: 4.7830e-04\n",
      "Epoch 200/200\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3454 - categorical_accuracy: 0.8566 - precision_2: 0.9128 - recall_2: 0.8242 - val_loss: 0.7573 - val_categorical_accuracy: 0.7948 - val_precision_2: 0.8440 - val_recall_2: 0.7608 - lr: 4.7830e-04\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Training\n",
    "'''\n",
    "\n",
    "model_path = r'D:\\voice\\model'\n",
    "\n",
    "checkpointer = ModelCheckpoint(model_path+'/'+'voice_model1.h5', save_best_only=True, monitor='val_loss')\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.000001)\n",
    "model_training = model.fit(x_train, y_train,\n",
    "                           batch_size=64,\n",
    "                           epochs=200,\n",
    "                           validation_data=(x_test, y_test),\n",
    "                           callbacks=[checkpointer, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "997873b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 566us/step\n",
      "0.7947845804988662\n",
      "           p_happy  p_ps  p_neutral  p_sad  p_angry\n",
      "t_happy        130    34         11     13        7\n",
      "t_ps            11   159         11     14        1\n",
      "t_neutral        0     1         88     10        0\n",
      "t_sad            5    11         11    166        3\n",
      "t_angry         14    17          3      4      158\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Checking accuracy score and confusion matrix\n",
    "'''\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "y_test = [np.argmax(i, out=None, axis=None) for i in y_test]\n",
    "\n",
    "print(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "emotions = ['happy', 'ps', 'neutral', 'sad', 'angry']\n",
    "emotions2int={'happy': 0, 'ps': 1, 'neutral': 2, 'sad': 3, 'angry': 4}\n",
    "matrix = confusion_matrix(y_test, y_pred,\n",
    "                          labels=[emotions2int[e] for e in emotions])\n",
    "matrix = pd.DataFrame(matrix, index=[f\"t_{e}\" for e in emotions],columns=[f\"p_{e}\" for e in emotions])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7d21c",
   "metadata": {},
   "source": [
    "预测FOMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b434879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df):\n",
    "    df = df.drop([col for col in df.columns if \"item\" in col], axis=1)\n",
    "    # Uncomment to drop a feature\n",
    "##    df = df.drop([col for col in df.columns if \"mfccs\" in col], axis=1)\n",
    "##    df = df.drop([col for col in df.columns if \"chroma\" in col], axis=1)\n",
    "##    df = df.drop([col for col in df.columns if \"mel\" in col], axis=1)\n",
    "    df = df.drop([col for col in df.columns if \"contrast\" in col], axis=1)\n",
    "    df = df.drop([col for col in df.columns if \"tonnetz\" in col], axis=1)\n",
    "\n",
    "    x_pred = df.to_numpy()\n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47c803e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_n</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ps</td>\n",
       "      <td>20110622_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ps</td>\n",
       "      <td>20110622_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ps</td>\n",
       "      <td>20110622_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ps</td>\n",
       "      <td>20110622_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ps</td>\n",
       "      <td>20110622_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>angry</td>\n",
       "      <td>20190731_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>angry</td>\n",
       "      <td>20190731_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>angry</td>\n",
       "      <td>20190731_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>angry</td>\n",
       "      <td>20190731_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>angry</td>\n",
       "      <td>20190731_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion emotion_n        item\n",
       "0        1        ps  20110622_1\n",
       "1        1        ps  20110622_2\n",
       "2        1        ps  20110622_3\n",
       "3        1        ps  20110622_4\n",
       "4        1        ps  20110622_5\n",
       "5        4     angry  20190731_1\n",
       "6        4     angry  20190731_2\n",
       "7        4     angry  20190731_3\n",
       "8        4     angry  20190731_4\n",
       "9        4     angry  20190731_5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = r'D:\\voice\\feature_prediction.csv'\n",
    "outfile = r'D:\\voice\\result.csv'\n",
    "emotions = ['happy', 'ps', 'neutral', 'sad', 'angry']\n",
    "dictionary = {0: 'happy', 1: 'ps', 2: 'neutral', 3: 'sad', 4: 'angry'}\n",
    "df_pred=pd.read_csv(infile, sep='\\t')\n",
    "x_pred = processing_data(df_pred)\n",
    "\n",
    "      \n",
    "model1 = keras.models.load_model(r'D:\\voice\\model\\voice_model.h5')\n",
    "y_pred = np.argmax(model1.predict(x_pred), axis=-1)\n",
    "output_data = pd.DataFrame(y_pred, columns = ['emotion'])\n",
    "output_data['emotion_n'] = output_data['emotion'].map(dictionary)\n",
    "\n",
    "\n",
    "output_data['item']=df_pred['item']\n",
    "output_data.to_csv(outfile, sep='\\t', index=False)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb67f3",
   "metadata": {},
   "source": [
    "原文："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273ca12",
   "metadata": {},
   "source": [
    "| Press Conference Date | Speaker   | Positive Responses | Neutral Responses | Negative Responses | Tone |\n",
    "|------------------------|-----------|---------------------|-------------------|--------------------|------|\n",
    "| June 22, 2011          | Bernanke  | 19                  | 0                 | 0                  | 1.00 |\n",
    "| July 31, 2019          | Powell    | 0                   | 0                 | 24                 | -1.00|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d4253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
